{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Fine-tune WangchanBERTa [Exp1: fine-tune].ipynb","provenance":[{"file_id":"1VwHo8gTBIwR-EQfMmTCEnzvGpJLTQkCw","timestamp":1635777473931},{"file_id":"10cvKYjXX__tjyByBPQVzyZjm2otyYShy","timestamp":1631707464118},{"file_id":"1gh-cr1UCdC6yAZd1oOxAnMhCUdBFhkB4","timestamp":1630822152939},{"file_id":"1hdFRWS-l9mQmL614VWgsUMw2ln0uuLyh","timestamp":1630690751565},{"file_id":"187rIuGjNJiFqXgLgZg8hbPVspntafDIZ","timestamp":1625408544629},{"file_id":"1PFtJQ_yIxQw_qJXIJhVQ8BdgOFtoVOMN","timestamp":1625391206435},{"file_id":"15eHqe3dQJw63mhVyCoeuhyxrS0eHMagX","timestamp":1624633400803}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ICAWf9vLw2j","executionInfo":{"status":"ok","timestamp":1638885817024,"user_tz":0,"elapsed":278,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"95b455e4-9825-4517-c575-3719caaaea48"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LL80zCUIOxsh","executionInfo":{"status":"ok","timestamp":1638883399783,"user_tz":0,"elapsed":184165,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"69e72dd2-7ed1-497d-f71d-409aa4a2f4a4"},"source":["# Install libs\n","!pip -q install torch==1.5.0\n","!pip -q install torchtext==0.6"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 752.0 MB 9.2 kB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 26.3 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"3KIzREva71h7","executionInfo":{"status":"ok","timestamp":1638883399784,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQYCxkCFycRm","executionInfo":{"status":"ok","timestamp":1638883511018,"user_tz":0,"elapsed":111240,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"47d38e82-34de-4019-acca-3702b8fbab20"},"source":["!pip -q install thai2transformers"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.3 MB 17.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 38.4 MB/s \n","\u001b[K     |████████████████████████████████| 170 kB 52.1 MB/s \n","\u001b[K     |████████████████████████████████| 11.0 MB 13.3 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 43.3 MB/s \n","\u001b[K     |████████████████████████████████| 8.7 MB 19.3 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 524 kB 38.1 MB/s \n","\u001b[K     |████████████████████████████████| 10.1 MB 50.6 MB/s \n","\u001b[K     |████████████████████████████████| 2.9 MB 40.4 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 40.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 36.5 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 52.7 MB/s \n","\u001b[K     |████████████████████████████████| 132 kB 49.0 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 267 kB/s \n","\u001b[K     |████████████████████████████████| 743 kB 35.8 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 45.8 MB/s \n","\u001b[K     |████████████████████████████████| 160 kB 52.0 MB/s \n","\u001b[K     |████████████████████████████████| 192 kB 53.5 MB/s \n","\u001b[K     |████████████████████████████████| 332 kB 37.8 MB/s \n","\u001b[K     |████████████████████████████████| 829 kB 54.3 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 41.6 MB/s \n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.4 MB/s eta 0:00:34tcmalloc: large alloc 1147494400 bytes == 0x557026b80000 @  0x7feaab24a615 0x5570210ae4cc 0x55702118e47a 0x5570210b12ed 0x5570211a2e1d 0x557021124e99 0x55702111f9ee 0x5570210b2bda 0x557021124d00 0x55702111f9ee 0x5570210b2bda 0x557021121737 0x5570211a3c66 0x557021120daf 0x5570211a3c66 0x557021120daf 0x5570211a3c66 0x557021120daf 0x5570210b3039 0x5570210f6409 0x5570210b1c52 0x557021124c25 0x55702111f9ee 0x5570210b2bda 0x557021121737 0x55702111f9ee 0x5570210b2bda 0x557021120915 0x5570210b2afa 0x557021120c0d 0x55702111f9ee\n","\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n","\u001b[K     |████████████████████████████████| 321 kB 33.8 MB/s \n","\u001b[?25h  Building wheel for thai2transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"WRBpkHFl3AQV","executionInfo":{"status":"ok","timestamp":1638883511018,"user_tz":0,"elapsed":24,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["fin = open(\"/usr/local/lib/python3.7/dist-packages/transformers/trainer_pt_utils.py\")\n","content = []\n","for line in fin:\n","  content.append(line)\n","fin.close()\n","\n","content[39] = \"    SAVE_STATE_WARNING = ''\\n\"\n","\n","fout = open(\"/usr/local/lib/python3.7/dist-packages/transformers/trainer_pt_utils.py\", \"w\")\n","for line in content:\n","  fout.write(line)\n","fout.close()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TKD9JcvO4xh","executionInfo":{"status":"ok","timestamp":1638883511019,"user_tz":0,"elapsed":19,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# from packaging import version\n","# import torch\n","# if version.parse(torch.__version__) <= version.parse(\"1.4.1\"):\n","#     SAVE_STATE_WARNING = \"\"\n","# else:\n","#     from torch.optim.lr_scheduler import SAVE_STATE_WARNING"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"dA0f5_Um2DPf","executionInfo":{"status":"ok","timestamp":1638883511019,"user_tz":0,"elapsed":17,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# !pip install -q pytorch-lightning"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5f9moE7PHpe","executionInfo":{"status":"ok","timestamp":1638883511020,"user_tz":0,"elapsed":17,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# !pip -q install git+https://github.com/PyTorchLightning/pytorch-lightning\n","# import pytorch_lightning as pl\n","# print(pl.__version__)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAR-Di_UPKGX","executionInfo":{"status":"ok","timestamp":1638883511020,"user_tz":0,"elapsed":17,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCFx-P4FQUaz"},"source":["# Load Pre-trained Model"]},{"cell_type":"code","metadata":{"id":"QDRhumG6NRoO","executionInfo":{"status":"ok","timestamp":1638885857852,"user_tz":0,"elapsed":365,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GUiDe3ye6Ks","executionInfo":{"status":"ok","timestamp":1638885855532,"user_tz":0,"elapsed":280,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"ec8d9330-3f13-4093-bb48-8e2f3d68257b"},"source":["cd /content/drive/MyDrive/Mispelling/misspelling-semantics/"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Mispelling/misspelling-semantics\n"]}]},{"cell_type":"code","metadata":{"id":"og7jEc7eL9FJ","executionInfo":{"status":"ok","timestamp":1638885860755,"user_tz":0,"elapsed":268,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["DIR = \"Models/WangchanBERTa-exp1\""],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubHucCA5QW93","executionInfo":{"status":"ok","timestamp":1638885861285,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["model_idx = 0\n","model_name = \"wangchanberta\"\n","# model_name = \"xlmr\"\n","# model_name = \"mbert\"\n","args_params = f\"{model_name} wisesight_sentiment {DIR}/Outputs/ {DIR}/Logs/ --batch_size 8 --seed {model_idx} --run_name exp1 --num_train_epochs 10\""],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-BVhvmoQXpk","executionInfo":{"status":"ok","timestamp":1638885864401,"user_tz":0,"elapsed":2461,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["import argparse\n","import math\n","import os\n","from functools import partial\n","import urllib.request\n","from tqdm import tqdm\n","from typing import Collection, Callable\n","from pathlib import Path\n","from sklearn import preprocessing\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","from transformers import (\n","    AdamW, \n","    get_linear_schedule_with_warmup, \n","    get_constant_schedule, \n","    AutoTokenizer, \n","    AutoModel,\n","    AutoModelForSequenceClassification, \n","    AutoConfig,\n","    Trainer, \n","    TrainingArguments,\n","    CamembertTokenizer,\n","    BertTokenizer,\n","    BertTokenizerFast,\n","    BertConfig,\n","    XLMRobertaTokenizer,\n","    XLMRobertaTokenizerFast,\n","    XLMRobertaConfig,\n","    DataCollatorWithPadding,\n","    default_data_collator\n",")\n","\n","from datasets import load_dataset, list_metrics, load_dataset, Dataset\n","from thai2transformers.datasets import SequenceClassificationDataset\n","from thai2transformers.metrics import classification_metrics, multilabel_classification_metrics\n","from thai2transformers.finetuners import SequenceClassificationFinetuner\n","from thai2transformers.auto import AutoModelForMultiLabelSequenceClassification\n","from thai2transformers.tokenizers import (\n","    ThaiRobertaTokenizer,\n","    ThaiWordsNewmmTokenizer,\n","    ThaiWordsSyllableTokenizer,\n","    FakeSefrCutTokenizer,\n",")\n","from thai2transformers.utils import get_dict_val\n","from thai2transformers.conf import Task\n","from thai2transformers import preprocess\n","\n","CACHE_DIR = f'{str(Path.home())}/.cache/huggingface_datasets'\n","\n","METRICS = {\n","    Task.MULTICLASS_CLS: classification_metrics,\n","    Task.MULTILABEL_CLS: multilabel_classification_metrics\n","}\n","\n","PUBLIC_MODEL = {\n","    # 'mbert': {\n","    #     'name': 'bert-base-multilingual-cased',\n","    #     'tokenizer': BertTokenizerFast.from_pretrained('bert-base-multilingual-cased'),\n","    #     'config': BertConfig.from_pretrained('bert-base-multilingual-cased'),\n","    # },\n","    'xlmr': {\n","        'name': 'xlm-roberta-base',\n","        'tokenizer': XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base'),\n","        'config': XLMRobertaConfig.from_pretrained('xlm-roberta-base'),\n","    },\n","    # 'xlmr-large': {\n","    #     'name': 'xlm-roberta-large',\n","    #     'tokenizer': XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large'),\n","    #     'config': XLMRobertaConfig.from_pretrained('xlm-roberta-base'),\n","    # },\n","    # 'thbert': {\n","    #     'name': 'monsoon-nlp/bert-base-thai',\n","    #     'tokenizer': AutoTokenizer.from_pretrained('monsoon-nlp/bert-base-thai'),\n","    #     'config': AutoConfig.from_pretrained('xlm-roberta-base'),\n","    # },\n","}\n","\n","TOKENIZER_CLS = {\n","    'wangchanberta': CamembertTokenizer,\n","    # 'spm': ThaiRobertaTokenizer,\n","    # 'newmm': ThaiWordsNewmmTokenizer,\n","    # 'syllable': ThaiWordsSyllableTokenizer,\n","    # 'sefr_cut': FakeSefrCutTokenizer,\n","}\n","\n","DATASET_METADATA = {\n","    'wisesight_sentiment': {\n","        'huggingface_dataset_name': 'wisesight_sentiment',\n","        'task': Task.MULTICLASS_CLS,\n","        'text_input_col_name': 'texts',\n","        'label_col_name': 'category',\n","        'num_labels': 3,\n","        'split_names': ['train', 'validation', 'test']\n","    }\n","}\n","\n","def init_public_model_tokenizer_for_seq_cls(public_model_name, task, num_labels):\n","    \n","    config = PUBLIC_MODEL[public_model_name]['config']\n","    config.num_labels = num_labels\n","    tokenizer = PUBLIC_MODEL[public_model_name]['tokenizer']\n","    model_name = PUBLIC_MODEL[public_model_name]['name']\n","    if task == Task.MULTICLASS_CLS:\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name,\n","                                                                   config=config)\n","    if task == Task.MULTILABEL_CLS:\n","        model = AutoModelForMultiLabelSequenceClassification.from_pretrained(model_name,\n","                                                                             config=config)\n","\n","    # print(f'\\n[INFO] Model architecture: {model} \\n\\n')\n","    # print(f'\\n[INFO] tokenizer: {tokenizer} \\n\\n')\n","\n","    return model, tokenizer, config\n","\n","def init_model_tokenizer_for_seq_cls(model_dir, tokenizer_cls, tokenizer_dir, task, num_labels):\n","    \n","    config = AutoConfig.from_pretrained(\n","        model_dir,\n","        num_labels=num_labels\n","    );\n","\n","    tokenizer = tokenizer_cls.from_pretrained(\n","        tokenizer_dir,\n","    );\n","\n","    if task == Task.MULTICLASS_CLS:\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_dir,\n","            config=config,\n","        )\n","    elif task == Task.MULTILABEL_CLS:\n","        model = AutoModelForMultiLabelSequenceClassification.from_pretrained(\n","            model_dir,\n","            config=config,\n","        )\n","\n","    # print(f'\\n[INFO] Model architecture: {model} \\n\\n')\n","    # print(f'\\n[INFO] tokenizer: {tokenizer} \\n\\n')\n","\n","    return model, tokenizer, config\n","\n","def init_trainer(task, model, train_dataset, val_dataset, warmup_steps, args, data_collator=default_data_collator): \n","        \n","    training_args = TrainingArguments(\n","                        num_train_epochs=args.num_train_epochs,\n","                        per_device_train_batch_size=args.batch_size,\n","                        per_device_eval_batch_size=args.batch_size,\n","                        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","                        learning_rate=args.learning_rate,\n","                        warmup_steps=warmup_steps,\n","                        weight_decay=args.weight_decay,\n","                        adam_epsilon=args.adam_epsilon,\n","                        max_grad_norm=args.max_grad_norm,\n","                        #checkpoint\n","                        output_dir=args.output_dir,\n","                        overwrite_output_dir=True,\n","                        #logs\n","                        logging_dir=args.log_dir,\n","                        logging_first_step=False,\n","                        logging_steps=args.logging_steps,\n","                        #eval\n","                        evaluation_strategy='epoch' if 'validation' in DATASET_METADATA[args.dataset_name]['split_names'] else 'no',\n","                        load_best_model_at_end=True,\n","                        #others\n","                        seed=args.seed,\n","                        fp16=args.fp16,\n","                        fp16_opt_level=args.fp16_opt_level,\n","                        dataloader_drop_last=False,\n","                        no_cuda=args.no_cuda,\n","                        metric_for_best_model=args.metric_for_best_model,\n","                        prediction_loss_only=False,\n","                        run_name=args.run_name\n","                    )\n","    if task == Task.MULTICLASS_CLS:\n","        compute_metrics_fn = METRICS[task]\n","    elif task == Task.MULTILABEL_CLS:\n","        compute_metrics_fn = partial(METRICS[task],n_labels=DATASET_METADATA[args.dataset_name]['num_labels'])\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        compute_metrics=compute_metrics_fn,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        data_collator=data_collator\n","    )\n","    return trainer, training_args\n","\n","# def _process_transformers(\n","#     text: str,\n","#     pre_rules: Collection[Callable] = [\n","#         preprocess.fix_html,\n","#         preprocess.rm_brackets,\n","#         preprocess.replace_newlines,\n","#         preprocess.rm_useless_spaces,\n","#         preprocess.replace_spaces,\n","#         preprocess.replace_rep_after,\n","#     ],\n","#     tok_func: Callable = preprocess.word_tokenize,\n","#     post_rules: Collection[Callable] = [preprocess.ungroup_emoji, preprocess.replace_wrep_post],\n","#     lowercase: bool = False\n","# ) -> str:\n","#     if lowercase:\n","#         text = text.lower()\n","#     for rule in pre_rules:\n","#         text = rule(text)\n","#     toks = tok_func(text)\n","#     for rule in post_rules:\n","#         toks = rule(toks)\n","#     return \"\".join(toks)"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anv_bUF0QZL2","executionInfo":{"status":"ok","timestamp":1638885864401,"user_tz":0,"elapsed":8,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"29da6c2f-84a8-48a3-c846-9b2358376eee"},"source":["parser = argparse.ArgumentParser()\n","# Required\n","parser.add_argument('tokenizer_type_or_public_model_name', type=str, help='The type token model used. Specify the name of tokenizer either `spm`, `newmm`, `syllable`, or `sefr_cut`.')\n","parser.add_argument('dataset_name', help='Specify the dataset name to finetune. Currently, sequence classification datasets include `wisesight_sentiment`, `generated_reviews_enth-correct_translation`, `generated_reviews_enth-review_star` and`wongnai_reviews`.')\n","parser.add_argument('output_dir', type=str)\n","parser.add_argument('log_dir', type=str)\n","\n","parser.add_argument('--model_dir', type=str)\n","parser.add_argument('--tokenizer_dir', type=str)\n","parser.add_argument('--prepare_for_tokenization', action='store_true', default=False, help='To replace space with a special token e.g. `<_>`. This may require for some pretrained models.')\n","parser.add_argument('--space_token', type=str, default=' ', help='The special token for space, specify if argumet: prepare_for_tokenization is applied')\n","parser.add_argument('--max_length', type=int, default=None)\n","parser.add_argument('--lowercase', action='store_true', default=False)\n","\n","# Finetuning\n","parser.add_argument('--num_train_epochs', type=int, default=5)\n","parser.add_argument('--learning_rate', type=float, default=1e-05)\n","parser.add_argument('--weight_decay', type=float, default=0.01)\n","parser.add_argument('--warmup_ratio', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=16)\n","parser.add_argument('--no_cuda', action='store_true', default=False)\n","parser.add_argument('--fp16', action='store_true', default=False)\n","parser.add_argument('--greater_is_better', action='store_true', default=True)\n","parser.add_argument('--metric_for_best_model', type=str, default='f1_micro')\n","parser.add_argument('--logging_steps', type=int, default=10)\n","parser.add_argument('--seed', type=int, default=2020)\n","parser.add_argument('--fp16_opt_level', type=str, default='O1')\n","parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n","parser.add_argument('--adam_epsilon', type=float, default=1e-08)\n","parser.add_argument('--max_grad_norm', type=float, default=1.0)\n","\n","# wandb\n","parser.add_argument('--run_name', type=str, default=None)"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--run_name'], dest='run_name', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help=None, metavar=None)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"qCeYWu0tQauI","executionInfo":{"status":"ok","timestamp":1638885864402,"user_tz":0,"elapsed":6,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# parser.add_argument('tokenizer_type_or_public_model_name', type=str, help='The type token model used. Specify the name of tokenizer either `spm`, `newmm`, `syllable`, or `sefr_cut`.')\n","# parser.add_argument('dataset_name', help='Specify the dataset name to finetune. Currently, sequence classification datasets include `wisesight_sentiment`, `generated_reviews_enth-correct_translation`, `generated_reviews_enth-review_star` and`wongnai_reviews`.')\n","# parser.add_argument('output_dir', type=str)\n","# parser.add_argument('log_dir', type=str)\n","args = parser.parse_args(args_params.split())\n","\n","# Set seed\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","torch.manual_seed(args.seed)\n","np.random.seed(args.seed)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrh47k-vQc92","executionInfo":{"status":"ok","timestamp":1638885864402,"user_tz":0,"elapsed":6,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"9faa7ffb-e41f-4087-82a8-7c73470b71e8"},"source":["\n","# try:\n","print(f'\\n\\n[INFO] Dataset: {args.dataset_name}')\n","print(f'\\n\\n[INFO] Huggingface\\'s dataset name: {DATASET_METADATA[args.dataset_name][\"huggingface_dataset_name\"]} ')\n","print(f'[INFO] Task: {DATASET_METADATA[args.dataset_name][\"task\"].value}')\n","print(f'\\n[INFO] space_token: {args.space_token}')\n","print(f'[INFO] prepare_for_tokenization: {args.prepare_for_tokenization}\\n')\n"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","[INFO] Dataset: wisesight_sentiment\n","\n","\n","[INFO] Huggingface's dataset name: wisesight_sentiment \n","[INFO] Task: multiclass_classification\n","\n","[INFO] space_token:  \n","[INFO] prepare_for_tokenization: False\n","\n"]}]},{"cell_type":"code","metadata":{"id":"SgzmOBvkQoTB","executionInfo":{"status":"ok","timestamp":1638885864402,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# dataset = load_dataset(DATASET_METADATA[args.dataset_name][\"huggingface_dataset_name\"])"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChlS02wkQqRr","executionInfo":{"status":"ok","timestamp":1638885864403,"user_tz":0,"elapsed":5,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# labels = {\n","#     \"neg\": 2,\n","#     \"neu\": 1,\n","#     \"pos\": 0,\n","#     \"q\": 3\n","# }\n","# dataset"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8AMunmmXv3V","executionInfo":{"status":"ok","timestamp":1638885864403,"user_tz":0,"elapsed":5,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# dataset[\"train\"][0:5]"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nv7ywhlqWau3","executionInfo":{"status":"ok","timestamp":1638885870055,"user_tz":0,"elapsed":5657,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"c9e02531-f0a3-440d-8bd7-a66df1ebf850"},"source":["text_input_col_name = DATASET_METADATA[args.dataset_name]['text_input_col_name']\n","\n","if args.tokenizer_type_or_public_model_name not in list(TOKENIZER_CLS.keys()) \\\n","    and args.tokenizer_type_or_public_model_name not in list(PUBLIC_MODEL.keys()):\n","    raise f\"The tokenizer type or public model name `{args.tokenizer_type_or_public_model_name}`` is not supported\"\n","\n","if args.tokenizer_type_or_public_model_name in list(TOKENIZER_CLS.keys()):\n","    tokenizer_cls = TOKENIZER_CLS[args.tokenizer_type_or_public_model_name]\n","\n","\n","task = DATASET_METADATA[args.dataset_name]['task']\n","if args.tokenizer_type_or_public_model_name in PUBLIC_MODEL.keys():\n","    print(args.tokenizer_type_or_public_model_name)\n","    model, tokenizer, config = init_public_model_tokenizer_for_seq_cls(args.tokenizer_type_or_public_model_name,\n","                                                        task=task,\n","                                                        num_labels=DATASET_METADATA[args.dataset_name]['num_labels']);\n","else:\n","    print(\"WangchanBERTa\")\n","    model, tokenizer, config = init_model_tokenizer_for_seq_cls(\"airesearch/wangchanberta-base-att-spm-uncased\",\n","                                                        tokenizer_cls,\n","                                                        \"airesearch/wangchanberta-base-att-spm-uncased\",\n","                                                        task=task,\n","                                                        num_labels=DATASET_METADATA[args.dataset_name]['num_labels']);"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["WangchanBERTa\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"8dcSKA23WpQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638885870056,"user_tz":0,"elapsed":15,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"decfdb1e-127a-46b8-8d8f-6554815c5a92"},"source":["# if args.tokenizer_type_or_public_model_name == 'wangchanberta':\n","#     tokenizer.additional_special_tokens = ['<s>NOTUSED', '</s>NOTUSED', args.space_token]\n","\n","print('\\n[INFO] Preprocess and tokenizing texts in datasets')\n","max_length = args.max_length if args.max_length else config.max_position_embeddings\n","print(f'[INFO] max_length = {max_length} \\n')"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[INFO] Preprocess and tokenizing texts in datasets\n","[INFO] max_length = 512 \n","\n"]}]},{"cell_type":"code","metadata":{"id":"AfYFfObgad6q","executionInfo":{"status":"ok","timestamp":1638885870056,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# !pip -q install demoji\n","# import demoji\n","# demoji.download_codes()"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqsSdW8KbbFd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638885870056,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"e97259e7-b187-4890-c806-5d1c05758c9f"},"source":["ls"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mDatasets\u001b[0m/\n","'Fine-tune WangchanBERTa [Exp1: fine-tune].ipynb'\n","'Fine-tune WangchanBERTa [Exp2: MST].ipynb'\n","'Fine-tune WangchanBERTa [Exp3: few-shot].ipynb'\n","'Fine-tune WangchanBERTa [Exp4: few-shot+MST].ipynb'\n","'Misspelling Patterns.ipynb'\n"," \u001b[01;34mModels\u001b[0m/\n"," test_mispelling_correction.jsonl\n","'Tokenize Wisesight Data.ipynb'\n","'Train FastText.ipynb'\n","'Train LSTM on FastText [Exp1: FT].ipynb'\n","'Train LSTM on FastText [Exp2: VEC].ipynb'\n","'Train LSTM on FastText [Exp3: VEC+MST].ipynb'\n","'Train LSTM on FastText [Exp4: VEC-corr].ipynb'\n"," train_mispelling_dection.jsonl\n"]}]},{"cell_type":"markdown","metadata":{"id":"2mTebN_FOhqa"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"rBcBdWSQOjP2","executionInfo":{"status":"ok","timestamp":1638885870563,"user_tz":0,"elapsed":3,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["import json\n","import pandas as pd\n","\n","def load_jsonl(fname):\n","    fin = open(fname, encoding=\"utf-8\")\n","    data = []\n","    for line in fin:\n","        d = json.loads(line.strip())\n","        data.append(d)\n","\n","    return data\n","\n","def save_jsonl(data, filename):\n","    with open(filename, \"w\", encoding=\"utf-8\") as fo:\n","        for idx, d in enumerate(data):\n","            fo.write(json.dumps(d, ensure_ascii=False))\n","            fo.write(\"\\n\")"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpjyO0OwOjSW","executionInfo":{"status":"ok","timestamp":1638885873785,"user_tz":0,"elapsed":3225,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["traindata = load_jsonl(f\"{DIR}/../../Datasets/WisesightSentiment/tokenized_train.jsonl\")\n","validdata = load_jsonl(f\"{DIR}/../../Datasets/WisesightSentiment/tokenized_valid.jsonl\")\n","testdata = load_jsonl(f\"{DIR}/../../Datasets/WisesightSentiment/tokenized_test-misp.jsonl\")"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTPLMhBQO4u3","executionInfo":{"status":"ok","timestamp":1638885873785,"user_tz":0,"elapsed":29,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"988f0bd5-d9e5-4c0b-f57b-31c22086c41e"},"source":["import itertools\n","def filterByMode(data, mode=None):\n","  output = []\n","  for sent in data:\n","    if mode is None:\n","      tokenized = [seg[0] for seg in sent[\"segments\"]]\n","    elif mode==\"corr\":\n","      tokenized = [seg[1] for seg in sent[\"segments\"]]\n","      if len(sent[\"misp_tokens\"])==0:\n","        continue\n","    else:\n","      tokenized = [seg[0] for seg in sent[\"segments\"]]\n","      if len(sent[\"misp_tokens\"])==0:\n","        continue\n","    \n","    tokenized = list(itertools.chain(*tokenized))\n","  \n","    output.append({\n","        \"category\": sent[\"category\"],\n","        \"text\": sent[\"text\"],\n","        \"tokenized\": tokenized,\n","        \"segments\": sent[\"segments\"]\n","    })\n","\n","  return output\n","\n","traindata\n","validdata\n","allTestdata = filterByMode(testdata)\n","corrTestdata = filterByMode(testdata, \"corr\")\n","mispTestdata = filterByMode(testdata, \"misp\")\n","len(allTestdata), len(corrTestdata), len(mispTestdata)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2671, 880, 880)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"xflkz27paVDG","executionInfo":{"status":"ok","timestamp":1638885873786,"user_tz":0,"elapsed":21,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ewn4a1gWPCKC","executionInfo":{"status":"ok","timestamp":1638885873786,"user_tz":0,"elapsed":20,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"a2f2632b-0c4b-4b43-8df2-9a0f4fdcaed6"},"source":["for sent in testdata[1:10]:\n","  print(sent)\n","  # break"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["{'category': 'neu', 'text': 'ครับ #phithanbkk', 'misp_tokens': [], 'tokenized': ['ครับ', ' ', '#phithanbkk'], 'segments': [[['ครับ', ' ', '#phithanbkk'], ['ครับ', ' ', '#phithanbkk']]]}\n","{'category': 'neg', 'text': 'การด่าไปเหมือนได้บรรเทาความเครียดเฉยๆ แต่บีทีเอส (รถไฟฟ้า) มันสำนึกมั้ย ก็ไม่อ่ะ 😕', 'misp_tokens': [{'corr': 'ไหม', 'misp': 'มั้ย', 'int': True, 's': 67, 't': 71}], 'tokenized': ['การ', 'ด่า', 'ไป', 'เหมือน', 'ได้', 'บรรเทา', 'ความ', 'เครียด', 'เฉย', 'ๆ', ' ', 'แต่', 'บีทีเอส', ' ', '(', 'รถ', 'ไฟฟ้า', ')', ' ', 'มัน', 'สำนึก', 'มั้ย', ' ', 'ก็', 'ไม่', 'อ่ะ', ' ', 'confused', ' ', 'face'], 'segments': [[['การ', 'ด่า', 'ไป', 'เหมือน', 'ได้', 'บรรเทา', 'ความ', 'เครียด', 'เฉย', 'ๆ', ' ', 'แต่', 'บีทีเอส', ' ', '(', 'รถ', 'ไฟฟ้า', ')', ' ', 'มัน', 'สำนึก'], ['การ', 'ด่า', 'ไป', 'เหมือน', 'ได้', 'บรรเทา', 'ความ', 'เครียด', 'เฉย', 'ๆ', ' ', 'แต่', 'บีทีเอส', ' ', '(', 'รถ', 'ไฟฟ้า', ')', ' ', 'มัน', 'สำนึก']], [['มั้ย'], ['ไหม']], [['ก็', 'ไม่', 'อ่ะ', ' ', 'confused', ' ', 'face'], ['ก็', 'ไม่', 'อ่ะ', ' ', 'confused', ' ', 'face']]]}\n","{'category': 'neu', 'text': 'Cf clarins 5 ขวด 2850', 'misp_tokens': [], 'tokenized': ['Cf', ' ', 'clarins', ' ', '5', ' ', 'ขวด', ' ', '2850'], 'segments': [[['Cf', ' ', 'clarins', ' ', '5', ' ', 'ขวด', ' ', '2850'], ['Cf', ' ', 'clarins', ' ', '5', ' ', 'ขวด', ' ', '2850']]]}\n","{'category': 'neu', 'text': 'ทานได้ค่ะ น้ำซุป MK ต้มมาจากหัวผักกาด ซีอิ้วขาว เกลือ แลน้ำตาลค่ะ', 'misp_tokens': [{'corr': 'และ', 'misp': 'แล', 'int': False, 's': 54, 't': 56}], 'tokenized': ['ทาน', 'ได้', 'ค่ะ', ' ', 'น้ำ', 'ซุป', ' ', 'MK', ' ', 'ต้ม', 'มา', 'จาก', 'หัว', 'ผักกาด', ' ', 'ซีอิ้วขาว', ' ', 'เกลือ', ' ', 'แล', 'น้ำตาล', 'ค่ะ'], 'segments': [[['ทาน', 'ได้', 'ค่ะ', ' ', 'น้ำ', 'ซุป', ' ', 'MK', ' ', 'ต้ม', 'มา', 'จาก', 'หัว', 'ผักกาด', ' ', 'ซีอิ้วขาว', ' ', 'เกลือ'], ['ทาน', 'ได้', 'ค่ะ', ' ', 'น้ำ', 'ซุป', ' ', 'MK', ' ', 'ต้ม', 'มา', 'จาก', 'หัว', 'ผักกาด', ' ', 'ซีอิ้วขาว', ' ', 'เกลือ']], [['แล'], ['และ']], [['น้ำตาล', 'ค่ะ'], ['น้ำตาล', 'ค่ะ']]]}\n","{'category': 'neu', 'text': 'เคล็ดลับที่ขาดไม่ได้ในการป้องกันผิวจากแสงแดด คือการทาครีมกันแดด สาวๆบ้างคนอาจจะคิดว่ามันไม่ใช่เรื่องสำคัญเท่าไหร่ แต่บอกเลยว่า ผิดมาก เพราะแสงแดดสมัยนี้แรงมาก และมีอนุภาพการทำลายผิวสูงมาก ถ้าไม่อยากให้ผิวเราถูกทำร้ายแบบซ้ำๆซาก ควรทาครีมกันแดดที่ดีมีคุณภาพอย่าง Eucerin Sun Age Repair Serum ปัจจุบันครีมกันแดดมีมากมายให้เราเลือกซื้อก็จริง แต่ก็อย่าลืมเลือกสิ่งที่ดีที่สุดให้ตัวเราอย่าง Eucerin Sun Acne oil Control ค่ะ', 'misp_tokens': [], 'tokenized': ['เคล็ด', 'ลับ', 'ที่', 'ขาด', 'ไม่', 'ได้', 'ใน', 'การ', 'ป้องกัน', 'ผิว', 'จาก', 'แสง', 'แดด', ' ', 'คือ', 'การ', 'ทา', 'ครีม', 'กัน', 'แดด', ' ', 'สาว', 'ๆ', 'บ้าง', 'คน', 'อาจ', 'จะ', 'คิด', 'ว่า', 'มัน', 'ไม่', 'ใช่', 'เรื่อง', 'สำคัญ', 'เท่า', 'ไหร่', ' ', 'แต่', 'บอก', 'เลย', 'ว่า', ' ', 'ผิด', 'มาก', ' ', 'เพราะ', 'แสง', 'แดด', 'สมัย', 'นี้', 'แรง', 'มาก', ' ', 'และ', 'มี', 'อนุภาพ', 'การ', 'ทำลาย', 'ผิว', 'สูง', 'มาก', ' ', 'ถ้า', 'ไม่', 'อยาก', 'ให้', 'ผิว', 'เรา', 'ถูก', 'ทำร้าย', 'แบบ', 'ซ้ำ', 'ๆ', 'ซาก', ' ', 'ควร', 'ทา', 'ครีม', 'กัน', 'แดด', 'ที่', 'ดี', 'มี', 'คุณภาพ', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Age', ' ', 'Repair', ' Serum', ' ', 'ปัจจุบัน', 'ครีม', 'กัน', 'แดด', 'มี', 'มากมาย', 'ให้', 'เรา', 'เลือก', 'ซื้อ', 'ก็', 'จริง', ' ', 'แต่', 'ก็', 'อย่า', 'ลืม', 'เลือก', 'สิ่ง', 'ที่', 'ดี', 'ที่สุด', 'ให้', 'ตัว', 'เรา', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Acne', ' ', 'oil', ' ', 'Control', ' ', 'ค่ะ'], 'segments': [[['เคล็ด', 'ลับ', 'ที่', 'ขาด', 'ไม่', 'ได้', 'ใน', 'การ', 'ป้องกัน', 'ผิว', 'จาก', 'แสง', 'แดด', ' ', 'คือ', 'การ', 'ทา', 'ครีม', 'กัน', 'แดด', ' ', 'สาว', 'ๆ', 'บ้าง', 'คน', 'อาจ', 'จะ', 'คิด', 'ว่า', 'มัน', 'ไม่', 'ใช่', 'เรื่อง', 'สำคัญ', 'เท่า', 'ไหร่', ' ', 'แต่', 'บอก', 'เลย', 'ว่า', ' ', 'ผิด', 'มาก', ' ', 'เพราะ', 'แสง', 'แดด', 'สมัย', 'นี้', 'แรง', 'มาก', ' ', 'และ', 'มี', 'อนุภาพ', 'การ', 'ทำลาย', 'ผิว', 'สูง', 'มาก', ' ', 'ถ้า', 'ไม่', 'อยาก', 'ให้', 'ผิว', 'เรา', 'ถูก', 'ทำร้าย', 'แบบ', 'ซ้ำ', 'ๆ', 'ซาก', ' ', 'ควร', 'ทา', 'ครีม', 'กัน', 'แดด', 'ที่', 'ดี', 'มี', 'คุณภาพ', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Age', ' ', 'Repair', ' Serum', ' ', 'ปัจจุบัน', 'ครีม', 'กัน', 'แดด', 'มี', 'มากมาย', 'ให้', 'เรา', 'เลือก', 'ซื้อ', 'ก็', 'จริง', ' ', 'แต่', 'ก็', 'อย่า', 'ลืม', 'เลือก', 'สิ่ง', 'ที่', 'ดี', 'ที่สุด', 'ให้', 'ตัว', 'เรา', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Acne', ' ', 'oil', ' ', 'Control', ' ', 'ค่ะ'], ['เคล็ด', 'ลับ', 'ที่', 'ขาด', 'ไม่', 'ได้', 'ใน', 'การ', 'ป้องกัน', 'ผิว', 'จาก', 'แสง', 'แดด', ' ', 'คือ', 'การ', 'ทา', 'ครีม', 'กัน', 'แดด', ' ', 'สาว', 'ๆ', 'บ้าง', 'คน', 'อาจ', 'จะ', 'คิด', 'ว่า', 'มัน', 'ไม่', 'ใช่', 'เรื่อง', 'สำคัญ', 'เท่า', 'ไหร่', ' ', 'แต่', 'บอก', 'เลย', 'ว่า', ' ', 'ผิด', 'มาก', ' ', 'เพราะ', 'แสง', 'แดด', 'สมัย', 'นี้', 'แรง', 'มาก', ' ', 'และ', 'มี', 'อนุภาพ', 'การ', 'ทำลาย', 'ผิว', 'สูง', 'มาก', ' ', 'ถ้า', 'ไม่', 'อยาก', 'ให้', 'ผิว', 'เรา', 'ถูก', 'ทำร้าย', 'แบบ', 'ซ้ำ', 'ๆ', 'ซาก', ' ', 'ควร', 'ทา', 'ครีม', 'กัน', 'แดด', 'ที่', 'ดี', 'มี', 'คุณภาพ', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Age', ' ', 'Repair', ' Serum', ' ', 'ปัจจุบัน', 'ครีม', 'กัน', 'แดด', 'มี', 'มากมาย', 'ให้', 'เรา', 'เลือก', 'ซื้อ', 'ก็', 'จริง', ' ', 'แต่', 'ก็', 'อย่า', 'ลืม', 'เลือก', 'สิ่ง', 'ที่', 'ดี', 'ที่สุด', 'ให้', 'ตัว', 'เรา', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Acne', ' ', 'oil', ' ', 'Control', ' ', 'ค่ะ']]]}\n","{'category': 'neu', 'text': \"สำหรับสูตรของผม คือ Jack Daniel's หวานซ่อนเปรี้ยวชื่อ Jack Yellow Life ส่วนประกอบ Jack Daniel's 2 ชอท น้ำเก๊กฮวย น้ำมะนาว ใบมิ้นท์ เกลือเล็กน้อย วิธีปรุง นำ JackDaniel's มาเขย่ากับน้ำเก๊กฮวย เจือด้วยน้ำมะนาวบางๆ ตกแต่งด้วยใบมิ้น เสริฟด้วยแก้วที่ทาเกลือไว้ที่ปากแก้ว รสชาติที่จะได้คือหวาน หอม ซ่อนเปรี้ยว ด้วยคอนเซ็ปว่านี่แหละคือชีวิต จะหวานอย่างเดียวก็จะเลี่ยนไป จะเปรี้ยวเกินไปก็ไม่ใช่เรื่อง จึงควรจะมีทั้งเปรี้ยว ทั้งหวานคละเคล้ากันไปครับ\", 'misp_tokens': [], 'tokenized': ['สำหรับ', 'สูตร', 'ของ', 'ผม', ' ', 'คือ', ' ', 'Jack Daniel', \"'\", 's', ' ', 'หวาน', 'ซ่อน', 'เปรี้ยว', 'ชื่อ', ' ', 'Jack Yellow', ' ', 'Life', ' ', 'ส่วน', 'ประกอบ', ' ', 'Jack Daniel', \"'\", 's', ' ', '2', ' ', 'ชอท', ' ', 'น้ำ', 'เก๊กฮวย', ' ', 'น้ำ', 'มะนาว', ' ', 'ใบ', 'มิ้นท์', ' ', 'เกลือ', 'เล็กน้อย', ' ', 'วิธี', 'ปรุง', ' ', 'นำ', ' ', 'JackDaniel', \"'\", 's', ' ', 'มา', 'เขย่า', 'กับ', 'น้ำ', 'เก๊กฮวย', ' ', 'เจือ', 'ด้วย', 'น้ำ', 'มะนาว', 'บาง', 'ๆ', ' ', 'ตกแต่ง', 'ด้วย', 'ใบมิ้น', ' ', 'เสริฟ', 'ด้วย', 'แก้ว', 'ที่', 'ทาเกลือ', 'ไว้', 'ที่', 'ปากแก้ว', ' ', 'รสชาติ', 'ที่', 'จะ', 'ได้', 'คือ', 'หวาน', ' ', 'หอม', ' ', 'ซ่อน', 'เปรี้ยว', ' ', 'ด้วย', 'คอนเซ็ป', 'ว่า', 'นี่', 'แหละ', 'คือ', 'ชีวิต', ' ', 'จะ', 'หวาน', 'อย่าง', 'เดียว', 'ก็', 'จะ', 'เลี่ยน', 'ไป', ' ', 'จะ', 'เปรี้ยว', 'เกิน', 'ไป', 'ก็', 'ไม่', 'ใช่', 'เรื่อง', ' ', 'จึง', 'ควร', 'จะ', 'มี', 'ทั้ง', 'เปรี้ยว', ' ', 'ทั้งหวาน', 'คละเคล้า', 'กัน', 'ไป', 'ครับ'], 'segments': [[['สำหรับ', 'สูตร', 'ของ', 'ผม', ' ', 'คือ', ' ', 'Jack Daniel', \"'\", 's', ' ', 'หวาน', 'ซ่อน', 'เปรี้ยว', 'ชื่อ', ' ', 'Jack Yellow', ' ', 'Life', ' ', 'ส่วน', 'ประกอบ', ' ', 'Jack Daniel', \"'\", 's', ' ', '2', ' ', 'ชอท', ' ', 'น้ำ', 'เก๊กฮวย', ' ', 'น้ำ', 'มะนาว', ' ', 'ใบ', 'มิ้นท์', ' ', 'เกลือ', 'เล็กน้อย', ' ', 'วิธี', 'ปรุง', ' ', 'นำ', ' ', 'JackDaniel', \"'\", 's', ' ', 'มา', 'เขย่า', 'กับ', 'น้ำ', 'เก๊กฮวย', ' ', 'เจือ', 'ด้วย', 'น้ำ', 'มะนาว', 'บาง', 'ๆ', ' ', 'ตกแต่ง', 'ด้วย', 'ใบมิ้น', ' ', 'เสริฟ', 'ด้วย', 'แก้ว', 'ที่', 'ทาเกลือ', 'ไว้', 'ที่', 'ปากแก้ว', ' ', 'รสชาติ', 'ที่', 'จะ', 'ได้', 'คือ', 'หวาน', ' ', 'หอม', ' ', 'ซ่อน', 'เปรี้ยว', ' ', 'ด้วย', 'คอนเซ็ป', 'ว่า', 'นี่', 'แหละ', 'คือ', 'ชีวิต', ' ', 'จะ', 'หวาน', 'อย่าง', 'เดียว', 'ก็', 'จะ', 'เลี่ยน', 'ไป', ' ', 'จะ', 'เปรี้ยว', 'เกิน', 'ไป', 'ก็', 'ไม่', 'ใช่', 'เรื่อง', ' ', 'จึง', 'ควร', 'จะ', 'มี', 'ทั้ง', 'เปรี้ยว', ' ', 'ทั้งหวาน', 'คละเคล้า', 'กัน', 'ไป', 'ครับ'], ['สำหรับ', 'สูตร', 'ของ', 'ผม', ' ', 'คือ', ' ', 'Jack Daniel', \"'\", 's', ' ', 'หวาน', 'ซ่อน', 'เปรี้ยว', 'ชื่อ', ' ', 'Jack Yellow', ' ', 'Life', ' ', 'ส่วน', 'ประกอบ', ' ', 'Jack Daniel', \"'\", 's', ' ', '2', ' ', 'ชอท', ' ', 'น้ำ', 'เก๊กฮวย', ' ', 'น้ำ', 'มะนาว', ' ', 'ใบ', 'มิ้นท์', ' ', 'เกลือ', 'เล็กน้อย', ' ', 'วิธี', 'ปรุง', ' ', 'นำ', ' ', 'JackDaniel', \"'\", 's', ' ', 'มา', 'เขย่า', 'กับ', 'น้ำ', 'เก๊กฮวย', ' ', 'เจือ', 'ด้วย', 'น้ำ', 'มะนาว', 'บาง', 'ๆ', ' ', 'ตกแต่ง', 'ด้วย', 'ใบมิ้น', ' ', 'เสริฟ', 'ด้วย', 'แก้ว', 'ที่', 'ทาเกลือ', 'ไว้', 'ที่', 'ปากแก้ว', ' ', 'รสชาติ', 'ที่', 'จะ', 'ได้', 'คือ', 'หวาน', ' ', 'หอม', ' ', 'ซ่อน', 'เปรี้ยว', ' ', 'ด้วย', 'คอนเซ็ป', 'ว่า', 'นี่', 'แหละ', 'คือ', 'ชีวิต', ' ', 'จะ', 'หวาน', 'อย่าง', 'เดียว', 'ก็', 'จะ', 'เลี่ยน', 'ไป', ' ', 'จะ', 'เปรี้ยว', 'เกิน', 'ไป', 'ก็', 'ไม่', 'ใช่', 'เรื่อง', ' ', 'จึง', 'ควร', 'จะ', 'มี', 'ทั้ง', 'เปรี้ยว', ' ', 'ทั้งหวาน', 'คละเคล้า', 'กัน', 'ไป', 'ครับ']]]}\n","{'category': 'neg', 'text': 'เจ้ว่าการ์นิเย่แอบแรงนิสหน่อย เคยใช้โยเกิร์ตไม๊ พรุ้งนี้ลองพอกหน้าดูทำให้หน้าสบายขึ้น นุ่มขึ้น หายไวไวน๊าาาา', 'misp_tokens': [{'corr': 'นะ', 'misp': 'น๊าาาา', 'int': True, 's': 102, 't': 108}, {'corr': 'นิดหน่อย', 'misp': 'นิสหน่อย', 'int': True, 's': 21, 't': 29}, {'corr': 'ไหม', 'misp': 'ไม๊', 'int': True, 's': 44, 't': 47}], 'tokenized': ['เจ้ว่า', 'การ์นิเย่', 'แอบ', 'แรง', 'นิสหน่อย', ' ', 'เคย', 'ใช้', 'โยเกิร์ต', 'ไม๊', ' ', 'พรุ้ง', 'นี้', 'ลอง', 'พอก', 'หน้า', 'ดู', 'ทำ', 'ให้', 'หน้า', 'สบาย', 'ขึ้น', ' ', 'นุ่ม', 'ขึ้น', ' ', 'หาย', 'ไว', 'ไวน๊าาาา'], 'segments': [[['เจ้ว่า', 'การ์นิเย่', 'แอบ', 'แรง'], ['เจ้ว่า', 'การ์นิเย่', 'แอบ', 'แรง']], [['นิสหน่อย'], ['นิดหน่อย']], [['เคย', 'ใช้', 'โยเกิร์ต'], ['เคย', 'ใช้', 'โยเกิร์ต']], [['ไม๊'], ['ไหม']], [['พรุ้ง', 'นี้', 'ลอง', 'พอก', 'หน้า', 'ดู', 'ทำ', 'ให้', 'หน้า', 'สบาย', 'ขึ้น', ' ', 'นุ่ม', 'ขึ้น', ' ', 'หาย', 'ไวไว'], ['พรุ้ง', 'นี้', 'ลอง', 'พอก', 'หน้า', 'ดู', 'ทำ', 'ให้', 'หน้า', 'สบาย', 'ขึ้น', ' ', 'นุ่ม', 'ขึ้น', ' ', 'หาย', 'ไวไว']], [['น๊าาาา'], ['นะ']], [[], []]]}\n","{'category': 'neu', 'text': 'เอๅจริงๆถ้ๅมันเปิดให้ใช้และถูกกฎหมๅยจริวคงกลัวจะเก็บภๅษียังไงรึป่ๅว', 'misp_tokens': [{'corr': 'เอา', 'misp': 'เอๅ', 'int': False, 's': 0, 't': 3}, {'corr': 'ถ้า', 'misp': 'ถ้ๅ', 'int': False, 's': 8, 't': 11}, {'corr': 'กฎหมาย', 'misp': 'กฎหมๅย', 'int': False, 's': 30, 't': 36}, {'corr': 'จริง', 'misp': 'จริว', 'int': False, 's': 36, 't': 40}, {'corr': 'ฤๅษี', 'misp': 'ภๅษี', 'int': False, 's': 52, 't': 56}, {'corr': 'เปล่า', 'misp': 'ป่ๅว', 'int': False, 's': 63, 't': 67}], 'tokenized': ['เอๅจริง', 'ๆ', 'ถ้ๅ', 'มัน', 'เปิด', 'ให้', 'ใช้', 'และ', 'ถูก', 'กฎหมๅย', 'จริว', 'คง', 'กลัว', 'จะ', 'เก็บ', 'ภๅษี', 'ยัง', 'ไง', 'รึป่ๅว'], 'segments': [[[], []], [['เอๅ'], ['เอา']], [['จริง', 'ๆ'], ['จริง', 'ๆ']], [['ถ้ๅ'], ['ถ้า']], [['มัน', 'เปิด', 'ให้', 'ใช้', 'และ', 'ถูก'], ['มัน', 'เปิด', 'ให้', 'ใช้', 'และ', 'ถูก']], [['กฎหมๅย'], ['กฎหมาย']], [[], []], [['จริว'], ['จริง']], [['คง', 'กลัว', 'จะ', 'เก็บ'], ['คง', 'กลัว', 'จะ', 'เก็บ']], [['ภๅษี'], ['ฤๅษี']], [['ยัง', 'ไง', 'รึ'], ['ยัง', 'ไง', 'รึ']], [['ป่ๅว'], ['เปล่า']], [[], []]]}\n","{'category': 'neg', 'text': 'อิผ้าอนามัยเหิ้ย ติดทุกอย่าง ยกเว้นกางเกงใน', 'misp_tokens': [{'corr': 'อี', 'misp': 'อิ', 'int': True, 's': 0, 't': 2}, {'corr': 'เหี้ย', 'misp': 'เหิ้ย', 'int': False, 's': 11, 't': 16}], 'tokenized': ['อิผ้า', 'อนามัย', 'เหิ้ย', ' ', 'ติด', 'ทุก', 'อย่าง', ' ', 'ยกเว้น', 'กางเกง', 'ใน'], 'segments': [[[], []], [['อิ'], ['อี']], [['ผ้า', 'อนามัย'], ['ผ้า', 'อนามัย']], [['เหิ้ย'], ['เหี้ย']], [['ติด', 'ทุก', 'อย่าง', ' ', 'ยกเว้น', 'กางเกง', 'ใน'], ['ติด', 'ทุก', 'อย่าง', ' ', 'ยกเว้น', 'กางเกง', 'ใน']]]}\n"]}]},{"cell_type":"code","metadata":{"id":"NRWpDhQaitQc","executionInfo":{"status":"ok","timestamp":1638885873787,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"yceBl7D2jGyv","executionInfo":{"status":"ok","timestamp":1638885873787,"user_tz":0,"elapsed":9,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLTGbmXyf0gp","executionInfo":{"status":"ok","timestamp":1638885873788,"user_tz":0,"elapsed":9,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["import glob\n","from torch.utils.data import Dataset as TorchDataset\n","from datasets import Dataset\n","# from thai2transformers.datasets import SequenceClassificationDataset\n","\n","class SequenceClassificationDataset(TorchDataset):\n","    def __init__(\n","        self,\n","        tokenizer,\n","        data_dir,\n","        task=Task.MULTICLASS_CLS,\n","        max_length=128,\n","        ext=\".csv\",\n","        bs=10000,\n","        preprocessor=None,\n","        input_ids=[],\n","        misp_ids=[],\n","        attention_masks=[],\n","        labels=[],\n","        label_encoder=None\n","    ):\n","        self.max_length = max_length\n","        self.tokenizer = tokenizer\n","        self.bs = bs\n","        self.preprocessor = preprocessor\n","        self.input_ids = input_ids\n","        self.misp_ids = misp_ids\n","        self.attention_masks = attention_masks\n","        self.labels = labels\n","        self.task = task\n","        self.label_encoder = label_encoder\n","        # self._build()\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, i):\n","        return {\n","            \"input_ids\": torch.tensor(self.input_ids[i], dtype=torch.long),\n","            \"misp_ids\": torch.tensor(self.misp_ids[i], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(self.attention_masks[i], dtype=torch.long),\n","            \"label\": torch.tensor(self.labels[i], dtype=torch.long),\n","        }\n"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Zxb64qIt674","executionInfo":{"status":"ok","timestamp":1638885873788,"user_tz":0,"elapsed":8,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxC53Pw1zQ-V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638885893792,"user_tz":0,"elapsed":20012,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"af0bbeeb-da4d-4bbe-b446-a7778b6f155b"},"source":["from tqdm import tqdm \n","import itertools\n","\n","LABELS = {\n","    \"neg\": 2,\n","    \"neu\": 1,\n","    \"pos\": 0,\n","    \"q\": 1 # used to be 3\n","}\n","\n","class CustomLabelEncoder():\n","    def __init__(self):\n","        pass\n","\n","    def transform(self, labels):\n","        return [LABELS[l] for l in labels]\n","    \n","\n","def remove_starting_marker(t, unk):\n","    if len(t) > 0:\n","      if t[0]=='▁':\n","        t = t[1:]\n","      elif t[0].startswith('▁'):        \n","        if tokenizer.convert_tokens_to_ids([t[0][1:]])[0] != unk:\n","          t[0] = t[0][1:]\n","    return t\n","\n","def preprocessing(d, preSegmented=False, mode=None):\n","    max_length = 400\n","    custom_label_encoder = CustomLabelEncoder()\n","    labels = get_dict_val(d, \"category\")\n","\n","    labels = custom_label_encoder.transform(labels)\n","\n","    input_ids = []\n","    misp_ids = []\n","    attention_masks = []\n","    unk = tokenizer.convert_tokens_to_ids([\"<unk>\"])[0]\n","\n","    sents = []\n","    if not preSegmented:\n","      texts = get_dict_val(d, \"tokenized\")\n","      for tokens in tqdm(texts):\n","        tokens = [(t, t) for t in tokens]\n","        sents.append(tokens)\n","        \n","    else:\n","      texts = get_dict_val(d, \"segments\")\n","      for segments in tqdm(texts):\n","        s = [list(zip(seg[0], seg[1])) for seg in segments]\n","        tokens = list(itertools.chain(*s))\n","        sents.append(tokens)\n","\n","    for tokens in sents:\n","      # if mode is None, ignore corr\n","      misptokens = [t[0] for t in tokens]\n","      corrtokens = [t[0] for t in tokens]\n","      \n","      if mode==\"corr\":\n","        misptokens = [t[1] for t in tokens]\n","        corrtokens = [t[1] for t in tokens]\n","      elif mode==\"mae\":\n","        misptokens = [t[0] for t in tokens]\n","        corrtokens = [t[1] for t in tokens]\n","      \n","      \n","      midx = tokenizer.convert_tokens_to_ids(misptokens)\n","      cidx = tokenizer.convert_tokens_to_ids(corrtokens)\n","      assert(len(midx)==len(cidx))\n","\n","      newmisptokens = []\n","      newcorrtokens = []\n","      for i in range(len(midx)):\n","          if midx[i]==unk:\n","              t = tokenizer.tokenize(misptokens[i])\n","              t = remove_starting_marker(t, unk)\n","\n","              if misptokens[i]==corrtokens[i]:\n","                newmisptokens += t\n","                newcorrtokens += t\n","              else:\n","                newmisptokens += t\n","                tx = tokenizer.tokenize(corrtokens[i])\n","                tx = remove_starting_marker(tx, unk)\n","\n","                if len(tx) > 0:\n","                  newcorrtokens += [tx[0] for j in range(len(t))]\n","                else:\n","                  newcorrtokens += t\n","          else:\n","              newmisptokens.append(misptokens[i])\n","              newcorrtokens.append(corrtokens[i])\n","\n","      assert(len(newmisptokens)==len(newcorrtokens))\n","              \n","      # words = newwords\n","      newmisptokens = ['<s>'] + newmisptokens[0:max_length-2] + ['</s>']\n","      newcorrtokens = ['<s>'] + newcorrtokens[0:max_length-2] + ['</s>']\n","    \n","      midx = tokenizer.convert_tokens_to_ids(newmisptokens)\n","      cidx = tokenizer.convert_tokens_to_ids(newcorrtokens)\n","\n","      mask = [1 for i in midx]\n","        \n","      input_ids.append(midx)\n","      misp_ids.append(cidx)\n","      attention_masks.append(mask)\n","\n","    #   if len(input_ids) > 10:\n","    #     break\n","    \n","    # labels = labels[0:10]\n","\n","    return SequenceClassificationDataset(\n","        tokenizer=tokenizer,\n","        data_dir=None,\n","        max_length=max_length,\n","        input_ids=input_ids,\n","        misp_ids=misp_ids,\n","        attention_masks=attention_masks,\n","        labels=labels,\n","        task=task\n","    )\n","\n","raw_datasets = {\n","    \"train\": traindata,\n","    \"validation\": validdata,\n","    \"test\": allTestdata,\n","    \"test-corr\": corrTestdata,\n","    \"test-misp\": mispTestdata,\n","    \"test-mae\": mispTestdata,\n","    \"test-all-mae\": allTestdata,\n","}\n","\n","dataset_split = {}\n","for split_name in raw_datasets:\n","    print(\"Tokenizing:\", split_name)\n","    d = pd.DataFrame(raw_datasets[split_name])\n","    d = Dataset.from_pandas(d)\n","    preSegmented = split_name.startswith(\"test\")\n","    mode = None\n","    if \"corr\" in split_name:\n","      mode = \"corr\"\n","    elif \"misp\" in split_name:\n","      mode = \"misp\"\n","    elif \"mae\" in split_name:\n","      mode = \"mae\"\n","\n","    dataset_split[split_name] = preprocessing(d, preSegmented=preSegmented, mode=mode)\n","    # break"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing: train\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21628/21628 [00:00<00:00, 140195.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2404/2404 [00:00<00:00, 119246.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2671/2671 [00:00<00:00, 108279.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-corr\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 880/880 [00:00<00:00, 60197.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-misp\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 880/880 [00:00<00:00, 111385.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-mae\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 880/880 [00:00<00:00, 72355.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-all-mae\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2671/2671 [00:00<00:00, 106898.72it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"zTo1InxM7QlH","executionInfo":{"status":"ok","timestamp":1638885893793,"user_tz":0,"elapsed":43,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVpOylu-CrqE"},"source":["## Custom Classes for Our Experiments"]},{"cell_type":"code","metadata":{"id":"wHqjltTIEdzZ","executionInfo":{"status":"ok","timestamp":1638885893793,"user_tz":0,"elapsed":41,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["from transformers.modeling_roberta import RobertaEmbeddings"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Q5iZsOgEd2P","executionInfo":{"status":"ok","timestamp":1638885893794,"user_tz":0,"elapsed":41,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["from torch import nn\n","\n","class CustomRobertaEmbeddings(RobertaEmbeddings):\n","\n","    def __init__(self, ref, config):\n","        super().__init__(config)\n","        self.word_embeddings = ref.word_embeddings\n","\n","    def forward(self, input_ids, misp_ids, token_type_ids=None, position_ids=None, inputs_embeds=None):\n","\n","        input_shape = input_ids.size()\n","        seq_length = input_shape[1]\n","\n","        inputs_embeds = self.word_embeddings(input_ids)\n","        misp_embeds = self.word_embeddings(misp_ids)\n","\n","        embeddings = ((inputs_embeds + misp_embeds)*0.5)\n","        return embeddings\n"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL_oBhOdHTjt","executionInfo":{"status":"ok","timestamp":1638885893794,"user_tz":0,"elapsed":40,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["from transformers.modeling_roberta import RobertaModel"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ew6s3Y1f1vCa","executionInfo":{"status":"ok","timestamp":1638885893795,"user_tz":0,"elapsed":40,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["from transformers.modeling_camembert import CamembertForSequenceClassification\n","from transformers.modeling_roberta import RobertaForSequenceClassification\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","\n","class CustomSequenceClassification(CamembertForSequenceClassification):\n","    authorized_missing_keys = [r\"position_ids\"]\n","\n","    def __init__(self, config, refmodel=None):\n","        super().__init__(config)\n","        if refmodel is not None:\n","          config = refmodel.config\n","          # self.refmodel = refmodel\n","          self.num_labels = config.num_labels\n","\n","          self.roberta = refmodel.roberta\n","          self.classifier = refmodel.classifier\n","\n","          self.baseEmb = refmodel.roberta.embeddings\n","          self.newEmb = CustomRobertaEmbeddings(self.baseEmb, config)\n","\n","    def forward(self, *args, **kwargs):\n","        # del kwargs[\"misp_ids\"]\n","        # return self.refmodel(**kwargs)\n","\n","        return_dict = self.config.use_return_dict\n","\n","        inputs_embeds = self.newEmb(kwargs[\"input_ids\"], kwargs[\"misp_ids\"])\n","        \n","        # doc: https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel.forward\n","        outputs = self.roberta(\n","            input_ids=None,\n","            attention_mask=kwargs[\"attention_mask\"],\n","            token_type_ids=None,\n","            position_ids=None,\n","            head_mask=None,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        labels = kwargs[\"labels\"]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"lznRUPu1YJSA","executionInfo":{"status":"ok","timestamp":1638885893795,"user_tz":0,"elapsed":37,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# cusmodel = CustomSequenceClassification(model.config, model)\n","# trainer, training_args = init_trainer(task=task,\n","#                             model=cusmodel,\n","#                             train_dataset=dataset_split['train'],\n","#                             val_dataset=dataset_split['validation'] if 'validation' in DATASET_METADATA[args.dataset_name]['split_names'] else None,\n","#                             warmup_steps=warmup_steps,\n","#                             args=args,\n","#                             data_collator=data_collator)\n","\n","# p, label_ids, result = trainer.predict(test_dataset=dataset_split['test'])\n","\n","# print(f'Evaluation on test set (dataset: {args.dataset_name})')    \n","\n","# for key, value in result.items():\n","#     print(f'{key} : {value:.4f}')\n","\n","# # Evaluation on test set (dataset: wisesight_sentiment)\n","# # eval_loss : 1.0284\n","# # eval_accuracy : 0.3000\n","# # eval_f1_micro : 0.3000\n","# # eval_precision_micro : 0.3000\n","# # eval_recall_micro : 0.3000\n","# # eval_f1_macro : 0.1667\n","# # eval_precision_macro : 0.1667\n","# # eval_recall_macro : 0.1667\n","# # eval_nb_samples : 10.0000"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDP2AEpDCqXh","executionInfo":{"status":"ok","timestamp":1638885893795,"user_tz":0,"elapsed":36,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["from dataclasses import dataclass\n","from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n","from transformers import PreTrainedTokenizerBase\n","\n","@dataclass\n","class CustomDataCollatorWithPadding:\n","  tokenizer: PreTrainedTokenizerBase\n","  padding: Union[bool, str] = True\n","  max_length: Optional[int] = None\n","  pad_to_multiple_of: Optional[int] = None\n","  return_tensors: str = \"pt\"\n","\n","  def __call__(self, features):\n","    _tmpfeat = []\n","    for f in features:\n","      _tmpfeat.append({\n","          \"input_ids\": f[\"input_ids\"],\n","          \"attention_mask\": f[\"attention_mask\"],\n","          \"label\": f[\"label\"],\n","      })\n","\n","    batch = self.tokenizer.pad(\n","        _tmpfeat,\n","        padding=self.padding,\n","        max_length=self.max_length,\n","        pad_to_multiple_of=self.pad_to_multiple_of,\n","        # return_tensors=self.return_tensors,\n","    )\n","\n","    _tmpfeat = []\n","    for f in features:\n","      _tmpfeat.append({\n","          \"input_ids\": f[\"misp_ids\"],\n","          \"attention_mask\": f[\"attention_mask\"],\n","          \"label\": f[\"label\"],\n","      })\n","      \n","    mispbatch = self.tokenizer.pad(\n","        _tmpfeat,\n","        padding=self.padding,\n","        max_length=self.max_length,\n","        pad_to_multiple_of=self.pad_to_multiple_of,\n","        # return_tensors=self.return_tensors,\n","    )\n","\n","    # print(mispbatch[\"input_ids\"])\n","    # print(tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0]))\n","    # print(tokenizer.convert_ids_to_tokens(mispbatch[\"input_ids\"][0]))\n","    batch[\"misp_ids\"] = mispbatch[\"input_ids\"]\n","    assert(batch[\"misp_ids\"].shape==batch[\"input_ids\"].shape)\n","    # assert()\n","    if \"label\" in batch:\n","        batch[\"labels\"] = batch[\"label\"]\n","        del batch[\"label\"]\n","    if \"label_ids\" in batch:\n","        batch[\"labels\"] = batch[\"label_ids\"]\n","        del batch[\"label_ids\"]\n","    return batch"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_D7scYkC0Jg"},"source":["# Model Training "]},{"cell_type":"code","metadata":{"id":"Dx6AsnFmeQlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638885899380,"user_tz":0,"elapsed":5620,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"729ec92f-4ddb-4e62-ea06-0e8dce9050da"},"source":["warmup_steps = math.ceil(len(dataset_split['train']) / args.batch_size * args.warmup_ratio * args.num_train_epochs)\n","\n","print(f'\\n[INFO] Number of train examples = {len(raw_datasets[\"train\"])}')\n","print(f'[INFO] Number of batches per epoch (training set) = {math.ceil(len(dataset_split[\"train\"]) / args.batch_size)}')\n","\n","print(f'[INFO] Warmup ratio = {args.warmup_ratio}')\n","print(f'[INFO] Warmup steps = {warmup_steps}')\n","print(f'[INFO] Learning rate: {args.learning_rate}')\n","print(f'[INFO] Logging steps: {args.logging_steps}')\n","print(f'[INFO] FP16 training: {args.fp16}\\n')\n","\n","# if 'validation' in DATASET_METADATA[args.dataset_name]['split_names']:\n","print(f'[INFO] Number of validation examples = {len(raw_datasets[\"validation\"])}')\n","print(f'[INFO] Number of batches per epoch (validation set) = {math.ceil(len(dataset_split[\"validation\"]))}')\n","\n","data_collator = CustomDataCollatorWithPadding(tokenizer,\n","                                        padding=True,\n","                                        pad_to_multiple_of=8 if args.fp16 else None)\n","\n","cusmodel = CustomSequenceClassification(model.config, model)\n","trainer, training_args = init_trainer(task=task,\n","                            model=cusmodel,\n","                            train_dataset=dataset_split['train'],\n","                            val_dataset=dataset_split['validation'] if 'validation' in DATASET_METADATA[args.dataset_name]['split_names'] else None,\n","                            warmup_steps=warmup_steps,\n","                            args=args,\n","                            data_collator=data_collator)\n","\n","print('[INFO] TrainingArguments:')\n","print(training_args)\n","print('\\n')"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[INFO] Number of train examples = 21628\n","[INFO] Number of batches per epoch (training set) = 2704\n","[INFO] Warmup ratio = 0.1\n","[INFO] Warmup steps = 2704\n","[INFO] Learning rate: 1e-05\n","[INFO] Logging steps: 10\n","[INFO] FP16 training: False\n","\n","[INFO] Number of validation examples = 2404\n","[INFO] Number of batches per epoch (validation set) = 2404\n","[INFO] TrainingArguments:\n","TrainingArguments(output_dir='Models/WangchanBERTa-exp1/Outputs/', overwrite_output_dir=True, do_train=False, do_eval=None, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, warmup_steps=2704, logging_dir='Models/WangchanBERTa-exp1/Logs/', logging_first_step=False, logging_steps=10, save_steps=500, save_total_limit=None, no_cuda=False, seed=0, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='exp1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='f1_micro', greater_is_better=True)\n","\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":524},"id":"O8TTQ7VmQGhS","executionInfo":{"status":"ok","timestamp":1638896928361,"user_tz":0,"elapsed":11028994,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"414b6da2-9b86-4247-b29a-b21ad1bbeb13"},"source":["print('\\nBegin model finetuning.')\n","trainer.train()\n","print('Done.\\n')"],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Begin model finetuning.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='27040' max='27040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [27040/27040 3:03:48, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Micro</th>\n","      <th>Precision Micro</th>\n","      <th>Recall Micro</th>\n","      <th>F1 Macro</th>\n","      <th>Precision Macro</th>\n","      <th>Recall Macro</th>\n","      <th>Nb Samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.757764</td>\n","      <td>0.719646</td>\n","      <td>0.688852</td>\n","      <td>0.688852</td>\n","      <td>0.688852</td>\n","      <td>0.688852</td>\n","      <td>0.580085</td>\n","      <td>0.628104</td>\n","      <td>0.586775</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.718799</td>\n","      <td>0.673267</td>\n","      <td>0.730449</td>\n","      <td>0.730449</td>\n","      <td>0.730449</td>\n","      <td>0.730449</td>\n","      <td>0.651795</td>\n","      <td>0.688441</td>\n","      <td>0.640732</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.566553</td>\n","      <td>0.685787</td>\n","      <td>0.727121</td>\n","      <td>0.727121</td>\n","      <td>0.727121</td>\n","      <td>0.727121</td>\n","      <td>0.675026</td>\n","      <td>0.685496</td>\n","      <td>0.669162</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.480957</td>\n","      <td>0.715820</td>\n","      <td>0.735857</td>\n","      <td>0.735857</td>\n","      <td>0.735857</td>\n","      <td>0.735857</td>\n","      <td>0.674827</td>\n","      <td>0.700136</td>\n","      <td>0.664326</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.575000</td>\n","      <td>0.739281</td>\n","      <td>0.734193</td>\n","      <td>0.734193</td>\n","      <td>0.734193</td>\n","      <td>0.734193</td>\n","      <td>0.681811</td>\n","      <td>0.700168</td>\n","      <td>0.670077</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.332324</td>\n","      <td>0.854342</td>\n","      <td>0.730865</td>\n","      <td>0.730865</td>\n","      <td>0.730865</td>\n","      <td>0.730865</td>\n","      <td>0.681689</td>\n","      <td>0.695411</td>\n","      <td>0.671919</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.360938</td>\n","      <td>0.995471</td>\n","      <td>0.728369</td>\n","      <td>0.728369</td>\n","      <td>0.728369</td>\n","      <td>0.728369</td>\n","      <td>0.679320</td>\n","      <td>0.697520</td>\n","      <td>0.666292</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.437305</td>\n","      <td>1.100396</td>\n","      <td>0.725458</td>\n","      <td>0.725458</td>\n","      <td>0.725458</td>\n","      <td>0.725458</td>\n","      <td>0.686250</td>\n","      <td>0.688778</td>\n","      <td>0.683860</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.236035</td>\n","      <td>1.192604</td>\n","      <td>0.721714</td>\n","      <td>0.721714</td>\n","      <td>0.721714</td>\n","      <td>0.721714</td>\n","      <td>0.680441</td>\n","      <td>0.684677</td>\n","      <td>0.676589</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.204980</td>\n","      <td>1.252389</td>\n","      <td>0.726705</td>\n","      <td>0.726705</td>\n","      <td>0.726705</td>\n","      <td>0.726705</td>\n","      <td>0.684680</td>\n","      <td>0.689146</td>\n","      <td>0.680740</td>\n","      <td>2404</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at Models/WangchanBERTa-exp1/Outputs/checkpoint-10816 were not used when initializing CustomSequenceClassification: ['baseEmb.position_ids', 'baseEmb.word_embeddings.weight', 'baseEmb.position_embeddings.weight', 'baseEmb.token_type_embeddings.weight', 'baseEmb.LayerNorm.weight', 'baseEmb.LayerNorm.bias', 'newEmb.position_ids', 'newEmb.word_embeddings.weight', 'newEmb.position_embeddings.weight', 'newEmb.token_type_embeddings.weight', 'newEmb.LayerNorm.weight', 'newEmb.LayerNorm.bias']\n","- This IS expected if you are initializing CustomSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Done.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"K63mJONBgbiB","executionInfo":{"status":"ok","timestamp":1638896938474,"user_tz":0,"elapsed":10121,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["trainer.save_model(f\"{DIR}/fine-tune-Exp1\")"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIwrxhH2cLsR"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"lRczslIwcNmh","executionInfo":{"status":"ok","timestamp":1638896938476,"user_tz":0,"elapsed":11,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-iWxTbYZKh4","executionInfo":{"status":"ok","timestamp":1638896941807,"user_tz":0,"elapsed":3340,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["# assert(torch.equal(model.classifier.dense.weight, trainer.model.classifier.dense.weight))\n","trainer.model = CustomSequenceClassification(model.config, model)"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjcp02iSgYgC","executionInfo":{"status":"ok","timestamp":1638896941808,"user_tz":0,"elapsed":23,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":["trainer.model.eval();"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lAOts0QgwKba","executionInfo":{"status":"ok","timestamp":1638897097531,"user_tz":0,"elapsed":155744,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}},"outputId":"6a6b7709-db22-4fc5-dac5-8b75a84f932d"},"source":["\n","for split_name in dataset_split:\n","  if split_name.startswith(\"train\"):\n","    continue\n","\n","  p, label_ids, result = trainer.predict(test_dataset=dataset_split[split_name])\n","  print(f'Evaluation on {split_name}:')    \n","\n","  for key, value in result.items():\n","      print(f'{key} : {value:.4f}')\n","  \n","  print(\"*\"*40)\n","  print()"],"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='1299' max='301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [301/301 02:35]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation on validation:\n","eval_loss : 1.2524\n","eval_accuracy : 0.7267\n","eval_f1_micro : 0.7267\n","eval_precision_micro : 0.7267\n","eval_recall_micro : 0.7267\n","eval_f1_macro : 0.6847\n","eval_precision_macro : 0.6891\n","eval_recall_macro : 0.6807\n","eval_nb_samples : 2404.0000\n","****************************************\n","\n","Evaluation on test:\n","eval_loss : 1.2156\n","eval_accuracy : 0.7316\n","eval_f1_micro : 0.7316\n","eval_precision_micro : 0.7316\n","eval_recall_micro : 0.7316\n","eval_f1_macro : 0.6833\n","eval_precision_macro : 0.6898\n","eval_recall_macro : 0.6783\n","eval_nb_samples : 2671.0000\n","****************************************\n","\n","Evaluation on test-corr:\n","eval_loss : 1.2446\n","eval_accuracy : 0.7114\n","eval_f1_micro : 0.7114\n","eval_precision_micro : 0.7114\n","eval_recall_micro : 0.7114\n","eval_f1_macro : 0.6908\n","eval_precision_macro : 0.6962\n","eval_recall_macro : 0.6885\n","eval_nb_samples : 880.0000\n","****************************************\n","\n","Evaluation on test-misp:\n","eval_loss : 1.2371\n","eval_accuracy : 0.7205\n","eval_f1_micro : 0.7205\n","eval_precision_micro : 0.7205\n","eval_recall_micro : 0.7205\n","eval_f1_macro : 0.7033\n","eval_precision_macro : 0.7064\n","eval_recall_macro : 0.7022\n","eval_nb_samples : 880.0000\n","****************************************\n","\n","Evaluation on test-mae:\n","eval_loss : 1.2583\n","eval_accuracy : 0.7125\n","eval_f1_micro : 0.7125\n","eval_precision_micro : 0.7125\n","eval_recall_micro : 0.7125\n","eval_f1_macro : 0.6936\n","eval_precision_macro : 0.6968\n","eval_recall_macro : 0.6933\n","eval_nb_samples : 880.0000\n","****************************************\n","\n","Evaluation on test-all-mae:\n","eval_loss : 1.2226\n","eval_accuracy : 0.7289\n","eval_f1_micro : 0.7289\n","eval_precision_micro : 0.7289\n","eval_recall_micro : 0.7289\n","eval_f1_macro : 0.6798\n","eval_precision_macro : 0.6862\n","eval_recall_macro : 0.6752\n","eval_nb_samples : 2671.0000\n","****************************************\n","\n"]}]},{"cell_type":"code","metadata":{"id":"UZv8Xs1F674m","executionInfo":{"status":"ok","timestamp":1638897097533,"user_tz":0,"elapsed":41,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"bo7F93aYDNUQ","executionInfo":{"status":"ok","timestamp":1638897097533,"user_tz":0,"elapsed":38,"user":{"displayName":"Pakawat Nakwijit","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01430675152978466058"}}},"source":[""],"execution_count":67,"outputs":[]}]}