{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Fine-tune WangchanBERTa [Exp3: few-shot].ipynb","provenance":[{"file_id":"1VwHo8gTBIwR-EQfMmTCEnzvGpJLTQkCw","timestamp":1635777473931},{"file_id":"10cvKYjXX__tjyByBPQVzyZjm2otyYShy","timestamp":1631707464118},{"file_id":"1gh-cr1UCdC6yAZd1oOxAnMhCUdBFhkB4","timestamp":1630822152939},{"file_id":"1hdFRWS-l9mQmL614VWgsUMw2ln0uuLyh","timestamp":1630690751565},{"file_id":"187rIuGjNJiFqXgLgZg8hbPVspntafDIZ","timestamp":1625408544629},{"file_id":"1PFtJQ_yIxQw_qJXIJhVQ8BdgOFtoVOMN","timestamp":1625391206435},{"file_id":"15eHqe3dQJw63mhVyCoeuhyxrS0eHMagX","timestamp":1624633400803}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ICAWf9vLw2j","executionInfo":{"status":"ok","timestamp":1638886482114,"user_tz":0,"elapsed":372,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"5926feaa-5bc9-4eb1-84b1-9b1d94e94a40"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LL80zCUIOxsh","executionInfo":{"status":"ok","timestamp":1638883183761,"user_tz":0,"elapsed":42819,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"95cba611-00ab-45bc-8f35-40daa2f97124"},"source":["# Install libs\n","!pip -q install torch==1.5.0\n","!pip -q install torchtext==0.6"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"3KIzREva71h7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQYCxkCFycRm","executionInfo":{"status":"ok","timestamp":1638883296568,"user_tz":0,"elapsed":112809,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"a5cee0f1-76d4-4500-f104-5de4e3769c83"},"source":["!pip -q install thai2transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 25.0 MB/s \n","\u001b[K     |████████████████████████████████| 170 kB 39.8 MB/s \n","\u001b[K     |████████████████████████████████| 11.0 MB 47.2 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 46.7 MB/s \n","\u001b[K     |████████████████████████████████| 8.7 MB 9.1 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 524 kB 46.4 MB/s \n","\u001b[K     |████████████████████████████████| 10.1 MB 36.7 MB/s \n","\u001b[K     |████████████████████████████████| 2.9 MB 36.8 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 38.1 MB/s \n","\u001b[K     |████████████████████████████████| 132 kB 51.4 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 49.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 34.5 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 510 kB/s \n","\u001b[K     |████████████████████████████████| 743 kB 42.6 MB/s \n","\u001b[K     |████████████████████████████████| 160 kB 46.2 MB/s \n","\u001b[K     |████████████████████████████████| 192 kB 35.9 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 37.2 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 45.2 MB/s \n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 15.0 MB/s eta 0:00:04tcmalloc: large alloc 1147494400 bytes == 0x55bf820b6000 @  0x7f352afe1615 0x55bf7c9af4cc 0x55bf7ca8f47a 0x55bf7c9b22ed 0x55bf7caa3e1d 0x55bf7ca25e99 0x55bf7ca209ee 0x55bf7c9b3bda 0x55bf7ca25d00 0x55bf7ca209ee 0x55bf7c9b3bda 0x55bf7ca22737 0x55bf7caa4c66 0x55bf7ca21daf 0x55bf7caa4c66 0x55bf7ca21daf 0x55bf7caa4c66 0x55bf7ca21daf 0x55bf7c9b4039 0x55bf7c9f7409 0x55bf7c9b2c52 0x55bf7ca25c25 0x55bf7ca209ee 0x55bf7c9b3bda 0x55bf7ca22737 0x55bf7ca209ee 0x55bf7c9b3bda 0x55bf7ca21915 0x55bf7c9b3afa 0x55bf7ca21c0d 0x55bf7ca209ee\n","\u001b[K     |████████████████████████████████| 881.9 MB 20 kB/s \n","\u001b[K     |████████████████████████████████| 332 kB 49.0 MB/s \n","\u001b[K     |████████████████████████████████| 829 kB 45.0 MB/s \n","\u001b[K     |████████████████████████████████| 321 kB 50.5 MB/s \n","\u001b[?25h  Building wheel for thai2transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"WRBpkHFl3AQV"},"source":["fin = open(\"/usr/local/lib/python3.7/dist-packages/transformers/trainer_pt_utils.py\")\n","content = []\n","for line in fin:\n","  content.append(line)\n","fin.close()\n","\n","content[39] = \"    SAVE_STATE_WARNING = ''\\n\"\n","\n","fout = open(\"/usr/local/lib/python3.7/dist-packages/transformers/trainer_pt_utils.py\", \"w\")\n","for line in content:\n","  fout.write(line)\n","fout.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TKD9JcvO4xh"},"source":["# from packaging import version\n","# import torch\n","# if version.parse(torch.__version__) <= version.parse(\"1.4.1\"):\n","#     SAVE_STATE_WARNING = \"\"\n","# else:\n","#     from torch.optim.lr_scheduler import SAVE_STATE_WARNING"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dA0f5_Um2DPf"},"source":["# !pip install -q pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5f9moE7PHpe"},"source":["# !pip -q install git+https://github.com/PyTorchLightning/pytorch-lightning\n","# import pytorch_lightning as pl\n","# print(pl.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAR-Di_UPKGX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCFx-P4FQUaz"},"source":["# Load Pre-trained Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GUiDe3ye6Ks","executionInfo":{"status":"ok","timestamp":1638886490777,"user_tz":0,"elapsed":358,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"efc2e921-13b4-47c2-e650-784caaceac13"},"source":["cd /content/drive/MyDrive/Mispelling/misspelling-semantics/"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Mispelling/misspelling-semantics\n"]}]},{"cell_type":"code","metadata":{"id":"iN9UVXbep4Bx","executionInfo":{"status":"ok","timestamp":1638886492356,"user_tz":0,"elapsed":9,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"og7jEc7eL9FJ","executionInfo":{"status":"ok","timestamp":1638886492358,"user_tz":0,"elapsed":8,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["DIR = \"Models/WangchanBERTa-exp3\""],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubHucCA5QW93","executionInfo":{"status":"ok","timestamp":1638886496261,"user_tz":0,"elapsed":433,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["model_idx = 0\n","model_name = \"wangchanberta\"\n","# model_name = \"xlmr\"\n","# model_name = \"mbert\"\n","args_params = f\"{model_name} wisesight_sentiment {DIR}/Outputs/ {DIR}/Logs/ --batch_size 8 --seed {model_idx} --run_name exp1 --num_train_epochs 10\""],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-BVhvmoQXpk","executionInfo":{"status":"ok","timestamp":1638886499828,"user_tz":0,"elapsed":2987,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["import argparse\n","import math\n","import os\n","from functools import partial\n","import urllib.request\n","from tqdm import tqdm\n","from typing import Collection, Callable\n","from pathlib import Path\n","from sklearn import preprocessing\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","from transformers import (\n","    AdamW, \n","    get_linear_schedule_with_warmup, \n","    get_constant_schedule, \n","    AutoTokenizer, \n","    AutoModel,\n","    AutoModelForSequenceClassification, \n","    AutoConfig,\n","    Trainer, \n","    TrainingArguments,\n","    CamembertTokenizer,\n","    BertTokenizer,\n","    BertTokenizerFast,\n","    BertConfig,\n","    XLMRobertaTokenizer,\n","    XLMRobertaTokenizerFast,\n","    XLMRobertaConfig,\n","    DataCollatorWithPadding,\n","    default_data_collator\n",")\n","\n","from datasets import load_dataset, list_metrics, load_dataset, Dataset\n","from thai2transformers.datasets import SequenceClassificationDataset\n","from thai2transformers.metrics import classification_metrics, multilabel_classification_metrics\n","from thai2transformers.finetuners import SequenceClassificationFinetuner\n","from thai2transformers.auto import AutoModelForMultiLabelSequenceClassification\n","from thai2transformers.tokenizers import (\n","    ThaiRobertaTokenizer,\n","    ThaiWordsNewmmTokenizer,\n","    ThaiWordsSyllableTokenizer,\n","    FakeSefrCutTokenizer,\n",")\n","from thai2transformers.utils import get_dict_val\n","from thai2transformers.conf import Task\n","from thai2transformers import preprocess\n","\n","CACHE_DIR = f'{str(Path.home())}/.cache/huggingface_datasets'\n","\n","METRICS = {\n","    Task.MULTICLASS_CLS: classification_metrics,\n","    Task.MULTILABEL_CLS: multilabel_classification_metrics\n","}\n","\n","PUBLIC_MODEL = {\n","    # 'mbert': {\n","    #     'name': 'bert-base-multilingual-cased',\n","    #     'tokenizer': BertTokenizerFast.from_pretrained('bert-base-multilingual-cased'),\n","    #     'config': BertConfig.from_pretrained('bert-base-multilingual-cased'),\n","    # },\n","    'xlmr': {\n","        'name': 'xlm-roberta-base',\n","        'tokenizer': XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base'),\n","        'config': XLMRobertaConfig.from_pretrained('xlm-roberta-base'),\n","    },\n","    # 'xlmr-large': {\n","    #     'name': 'xlm-roberta-large',\n","    #     'tokenizer': XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large'),\n","    #     'config': XLMRobertaConfig.from_pretrained('xlm-roberta-base'),\n","    # },\n","    # 'thbert': {\n","    #     'name': 'monsoon-nlp/bert-base-thai',\n","    #     'tokenizer': AutoTokenizer.from_pretrained('monsoon-nlp/bert-base-thai'),\n","    #     'config': AutoConfig.from_pretrained('xlm-roberta-base'),\n","    # },\n","}\n","\n","TOKENIZER_CLS = {\n","    'wangchanberta': CamembertTokenizer,\n","    # 'spm': ThaiRobertaTokenizer,\n","    # 'newmm': ThaiWordsNewmmTokenizer,\n","    # 'syllable': ThaiWordsSyllableTokenizer,\n","    # 'sefr_cut': FakeSefrCutTokenizer,\n","}\n","\n","DATASET_METADATA = {\n","    'wisesight_sentiment': {\n","        'huggingface_dataset_name': 'wisesight_sentiment',\n","        'task': Task.MULTICLASS_CLS,\n","        'text_input_col_name': 'texts',\n","        'label_col_name': 'category',\n","        'num_labels': 3,\n","        'split_names': ['train', 'validation', 'test']\n","    }\n","}\n","\n","def init_public_model_tokenizer_for_seq_cls(public_model_name, task, num_labels):\n","    \n","    config = PUBLIC_MODEL[public_model_name]['config']\n","    config.num_labels = num_labels\n","    tokenizer = PUBLIC_MODEL[public_model_name]['tokenizer']\n","    model_name = PUBLIC_MODEL[public_model_name]['name']\n","    if task == Task.MULTICLASS_CLS:\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name,\n","                                                                   config=config)\n","    if task == Task.MULTILABEL_CLS:\n","        model = AutoModelForMultiLabelSequenceClassification.from_pretrained(model_name,\n","                                                                             config=config)\n","\n","    # print(f'\\n[INFO] Model architecture: {model} \\n\\n')\n","    # print(f'\\n[INFO] tokenizer: {tokenizer} \\n\\n')\n","\n","    return model, tokenizer, config\n","\n","def init_model_tokenizer_for_seq_cls(model_dir, tokenizer_cls, tokenizer_dir, task, num_labels):\n","    \n","    config = AutoConfig.from_pretrained(\n","        model_dir,\n","        num_labels=num_labels\n","    );\n","\n","    tokenizer = tokenizer_cls.from_pretrained(\n","        tokenizer_dir,\n","    );\n","\n","    if task == Task.MULTICLASS_CLS:\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_dir,\n","            config=config,\n","        )\n","    elif task == Task.MULTILABEL_CLS:\n","        model = AutoModelForMultiLabelSequenceClassification.from_pretrained(\n","            model_dir,\n","            config=config,\n","        )\n","\n","    # print(f'\\n[INFO] Model architecture: {model} \\n\\n')\n","    # print(f'\\n[INFO] tokenizer: {tokenizer} \\n\\n')\n","\n","    return model, tokenizer, config\n","\n","def init_trainer(task, model, train_dataset, val_dataset, warmup_steps, args, data_collator=default_data_collator): \n","        \n","    training_args = TrainingArguments(\n","                        num_train_epochs=args.num_train_epochs,\n","                        per_device_train_batch_size=args.batch_size,\n","                        per_device_eval_batch_size=args.batch_size,\n","                        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","                        learning_rate=args.learning_rate,\n","                        warmup_steps=warmup_steps,\n","                        weight_decay=args.weight_decay,\n","                        adam_epsilon=args.adam_epsilon,\n","                        max_grad_norm=args.max_grad_norm,\n","                        #checkpoint\n","                        output_dir=args.output_dir,\n","                        overwrite_output_dir=True,\n","                        #logs\n","                        logging_dir=args.log_dir,\n","                        logging_first_step=False,\n","                        logging_steps=args.logging_steps,\n","                        #eval\n","                        evaluation_strategy='epoch' if 'validation' in DATASET_METADATA[args.dataset_name]['split_names'] else 'no',\n","                        load_best_model_at_end=True,\n","                        #others\n","                        seed=args.seed,\n","                        fp16=args.fp16,\n","                        fp16_opt_level=args.fp16_opt_level,\n","                        dataloader_drop_last=False,\n","                        no_cuda=args.no_cuda,\n","                        metric_for_best_model=args.metric_for_best_model,\n","                        prediction_loss_only=False,\n","                        run_name=args.run_name\n","                    )\n","    if task == Task.MULTICLASS_CLS:\n","        compute_metrics_fn = METRICS[task]\n","    elif task == Task.MULTILABEL_CLS:\n","        compute_metrics_fn = partial(METRICS[task],n_labels=DATASET_METADATA[args.dataset_name]['num_labels'])\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        compute_metrics=compute_metrics_fn,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        data_collator=data_collator\n","    )\n","    return trainer, training_args\n","\n","# def _process_transformers(\n","#     text: str,\n","#     pre_rules: Collection[Callable] = [\n","#         preprocess.fix_html,\n","#         preprocess.rm_brackets,\n","#         preprocess.replace_newlines,\n","#         preprocess.rm_useless_spaces,\n","#         preprocess.replace_spaces,\n","#         preprocess.replace_rep_after,\n","#     ],\n","#     tok_func: Callable = preprocess.word_tokenize,\n","#     post_rules: Collection[Callable] = [preprocess.ungroup_emoji, preprocess.replace_wrep_post],\n","#     lowercase: bool = False\n","# ) -> str:\n","#     if lowercase:\n","#         text = text.lower()\n","#     for rule in pre_rules:\n","#         text = rule(text)\n","#     toks = tok_func(text)\n","#     for rule in post_rules:\n","#         toks = rule(toks)\n","#     return \"\".join(toks)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anv_bUF0QZL2","executionInfo":{"status":"ok","timestamp":1638886499829,"user_tz":0,"elapsed":6,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"788965fe-72c5-447b-a69f-1bc33c1d70cd"},"source":["parser = argparse.ArgumentParser()\n","# Required\n","parser.add_argument('tokenizer_type_or_public_model_name', type=str, help='The type token model used. Specify the name of tokenizer either `spm`, `newmm`, `syllable`, or `sefr_cut`.')\n","parser.add_argument('dataset_name', help='Specify the dataset name to finetune. Currently, sequence classification datasets include `wisesight_sentiment`, `generated_reviews_enth-correct_translation`, `generated_reviews_enth-review_star` and`wongnai_reviews`.')\n","parser.add_argument('output_dir', type=str)\n","parser.add_argument('log_dir', type=str)\n","\n","parser.add_argument('--model_dir', type=str)\n","parser.add_argument('--tokenizer_dir', type=str)\n","parser.add_argument('--prepare_for_tokenization', action='store_true', default=False, help='To replace space with a special token e.g. `<_>`. This may require for some pretrained models.')\n","parser.add_argument('--space_token', type=str, default=' ', help='The special token for space, specify if argumet: prepare_for_tokenization is applied')\n","parser.add_argument('--max_length', type=int, default=None)\n","parser.add_argument('--lowercase', action='store_true', default=False)\n","\n","# Finetuning\n","parser.add_argument('--num_train_epochs', type=int, default=5)\n","parser.add_argument('--learning_rate', type=float, default=1e-05)\n","parser.add_argument('--weight_decay', type=float, default=0.01)\n","parser.add_argument('--warmup_ratio', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=16)\n","parser.add_argument('--no_cuda', action='store_true', default=False)\n","parser.add_argument('--fp16', action='store_true', default=False)\n","parser.add_argument('--greater_is_better', action='store_true', default=True)\n","parser.add_argument('--metric_for_best_model', type=str, default='f1_micro')\n","parser.add_argument('--logging_steps', type=int, default=10)\n","parser.add_argument('--seed', type=int, default=2020)\n","parser.add_argument('--fp16_opt_level', type=str, default='O1')\n","parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n","parser.add_argument('--adam_epsilon', type=float, default=1e-08)\n","parser.add_argument('--max_grad_norm', type=float, default=1.0)\n","\n","# wandb\n","parser.add_argument('--run_name', type=str, default=None)"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--run_name'], dest='run_name', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help=None, metavar=None)"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"qCeYWu0tQauI","executionInfo":{"status":"ok","timestamp":1638886499830,"user_tz":0,"elapsed":5,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# parser.add_argument('tokenizer_type_or_public_model_name', type=str, help='The type token model used. Specify the name of tokenizer either `spm`, `newmm`, `syllable`, or `sefr_cut`.')\n","# parser.add_argument('dataset_name', help='Specify the dataset name to finetune. Currently, sequence classification datasets include `wisesight_sentiment`, `generated_reviews_enth-correct_translation`, `generated_reviews_enth-review_star` and`wongnai_reviews`.')\n","# parser.add_argument('output_dir', type=str)\n","# parser.add_argument('log_dir', type=str)\n","args = parser.parse_args(args_params.split())\n","\n","# Set seed\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","torch.manual_seed(args.seed)\n","np.random.seed(args.seed)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrh47k-vQc92","executionInfo":{"status":"ok","timestamp":1638886500206,"user_tz":0,"elapsed":381,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"bfd98a24-031e-46a0-a2ff-277796b3bc8c"},"source":["\n","# try:\n","print(f'\\n\\n[INFO] Dataset: {args.dataset_name}')\n","print(f'\\n\\n[INFO] Huggingface\\'s dataset name: {DATASET_METADATA[args.dataset_name][\"huggingface_dataset_name\"]} ')\n","print(f'[INFO] Task: {DATASET_METADATA[args.dataset_name][\"task\"].value}')\n","print(f'\\n[INFO] space_token: {args.space_token}')\n","print(f'[INFO] prepare_for_tokenization: {args.prepare_for_tokenization}\\n')\n"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","[INFO] Dataset: wisesight_sentiment\n","\n","\n","[INFO] Huggingface's dataset name: wisesight_sentiment \n","[INFO] Task: multiclass_classification\n","\n","[INFO] space_token:  \n","[INFO] prepare_for_tokenization: False\n","\n"]}]},{"cell_type":"code","metadata":{"id":"SgzmOBvkQoTB","executionInfo":{"status":"ok","timestamp":1638886500206,"user_tz":0,"elapsed":5,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# dataset = load_dataset(DATASET_METADATA[args.dataset_name][\"huggingface_dataset_name\"])"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChlS02wkQqRr","executionInfo":{"status":"ok","timestamp":1638886500206,"user_tz":0,"elapsed":5,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# labels = {\n","#     \"neg\": 2,\n","#     \"neu\": 1,\n","#     \"pos\": 0,\n","#     \"q\": 3\n","# }\n","# dataset"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8AMunmmXv3V","executionInfo":{"status":"ok","timestamp":1638886500207,"user_tz":0,"elapsed":6,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# dataset[\"train\"][0:5]"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nv7ywhlqWau3","executionInfo":{"status":"ok","timestamp":1638886509755,"user_tz":0,"elapsed":9554,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"b0e7ab1c-a7b4-4362-b229-318664ffa71b"},"source":["text_input_col_name = DATASET_METADATA[args.dataset_name]['text_input_col_name']\n","\n","if args.tokenizer_type_or_public_model_name not in list(TOKENIZER_CLS.keys()) \\\n","    and args.tokenizer_type_or_public_model_name not in list(PUBLIC_MODEL.keys()):\n","    raise f\"The tokenizer type or public model name `{args.tokenizer_type_or_public_model_name}`` is not supported\"\n","\n","if args.tokenizer_type_or_public_model_name in list(TOKENIZER_CLS.keys()):\n","    tokenizer_cls = TOKENIZER_CLS[args.tokenizer_type_or_public_model_name]\n","\n","\n","task = DATASET_METADATA[args.dataset_name]['task']\n","if args.tokenizer_type_or_public_model_name in PUBLIC_MODEL.keys():\n","    print(args.tokenizer_type_or_public_model_name)\n","    model, tokenizer, config = init_public_model_tokenizer_for_seq_cls(args.tokenizer_type_or_public_model_name,\n","                                                        task=task,\n","                                                        num_labels=DATASET_METADATA[args.dataset_name]['num_labels']);\n","else:\n","    print(\"WangchanBERTa\")\n","    model, tokenizer, config = init_model_tokenizer_for_seq_cls(\"airesearch/wangchanberta-base-att-spm-uncased\",\n","                                                        tokenizer_cls,\n","                                                        \"airesearch/wangchanberta-base-att-spm-uncased\",\n","                                                        task=task,\n","                                                        num_labels=DATASET_METADATA[args.dataset_name]['num_labels']);"],"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["WangchanBERTa\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"8dcSKA23WpQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638886509755,"user_tz":0,"elapsed":12,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"5fdad38a-bb66-4b45-de6f-0df1fc9e96fc"},"source":["# if args.tokenizer_type_or_public_model_name == 'wangchanberta':\n","#     tokenizer.additional_special_tokens = ['<s>NOTUSED', '</s>NOTUSED', args.space_token]\n","\n","print('\\n[INFO] Preprocess and tokenizing texts in datasets')\n","max_length = args.max_length if args.max_length else config.max_position_embeddings\n","print(f'[INFO] max_length = {max_length} \\n')"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[INFO] Preprocess and tokenizing texts in datasets\n","[INFO] max_length = 512 \n","\n"]}]},{"cell_type":"code","metadata":{"id":"AfYFfObgad6q","executionInfo":{"status":"ok","timestamp":1638886509756,"user_tz":0,"elapsed":10,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# !pip -q install demoji\n","# import demoji\n","# demoji.download_codes()"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqsSdW8KbbFd","executionInfo":{"status":"ok","timestamp":1638886509756,"user_tz":0,"elapsed":9,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mTebN_FOhqa"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"rBcBdWSQOjP2","executionInfo":{"status":"ok","timestamp":1638886509757,"user_tz":0,"elapsed":10,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["import json\n","import pandas as pd\n","\n","def load_jsonl(fname):\n","    fin = open(fname, encoding=\"utf-8\")\n","    data = []\n","    for line in fin:\n","        d = json.loads(line.strip())\n","        data.append(d)\n","\n","    return data\n","\n","def save_jsonl(data, filename):\n","    with open(filename, \"w\", encoding=\"utf-8\") as fo:\n","        for idx, d in enumerate(data):\n","            fo.write(json.dumps(d, ensure_ascii=False))\n","            fo.write(\"\\n\")"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZqL51pAmqAAw","executionInfo":{"status":"ok","timestamp":1638886509757,"user_tz":0,"elapsed":10,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"6c7ab1cb-4caa-452b-cd74-21df3c633cd1"},"source":["ls Datasets/WisesightSentiment"],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["neg.txt     test-misp.jsonl                  tokenized_valid.jsonl\n","neu.txt     tokenized_test.jsonl             train.jsonl\n","pos.txt     tokenized_test-misp.jsonl        valid.jsonl\n","q.txt       tokenized_train.jsonl\n","test.jsonl  tokenized_train-misp-3000.jsonl\n"]}]},{"cell_type":"code","metadata":{"id":"gpjyO0OwOjSW","executionInfo":{"status":"ok","timestamp":1638886511906,"user_tz":0,"elapsed":1760,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["traindata = load_jsonl(f\"{DIR}/../../Datasets/WisesightSentiment/tokenized_train-misp-3000.jsonl\")\n","validdata = load_jsonl(f\"{DIR}/../../Datasets/WisesightSentiment/tokenized_valid.jsonl\")\n","testdata = load_jsonl(f\"{DIR}/../../Datasets/WisesightSentiment/tokenized_test-misp.jsonl\")"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"CteXhrsSqkC8","executionInfo":{"status":"ok","timestamp":1638886511907,"user_tz":0,"elapsed":34,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# traindata[0][\"segments\"]"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTPLMhBQO4u3","executionInfo":{"status":"ok","timestamp":1638886511907,"user_tz":0,"elapsed":33,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"0943307c-f84d-4b99-ae59-334be99a0f13"},"source":["import itertools\n","def filterByMode(data, mode=None):\n","  output = []\n","  for sent in data:\n","    if mode is None:\n","      tokenized = [seg[0] for seg in sent[\"segments\"]]\n","    elif mode==\"corr\":\n","      tokenized = [seg[1] for seg in sent[\"segments\"]]\n","      if len(sent[\"misp_tokens\"])==0:\n","        continue\n","    else:\n","      tokenized = [seg[0] for seg in sent[\"segments\"]]\n","      if len(sent[\"misp_tokens\"])==0:\n","        continue\n","    \n","    tokenized = list(itertools.chain(*tokenized))\n","  \n","    output.append({\n","        \"category\": sent[\"category\"],\n","        \"text\": sent[\"text\"],\n","        \"tokenized\": tokenized,\n","        \"segments\": sent[\"segments\"]\n","    })\n","\n","  return output\n","\n","traindata\n","validdata\n","allTestdata = filterByMode(testdata)\n","corrTestdata = filterByMode(testdata, \"corr\")\n","mispTestdata = filterByMode(testdata, \"misp\")\n","len(allTestdata), len(corrTestdata), len(mispTestdata)"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2671, 880, 880)"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"xflkz27paVDG","executionInfo":{"status":"ok","timestamp":1638886511908,"user_tz":0,"elapsed":28,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ewn4a1gWPCKC","executionInfo":{"status":"ok","timestamp":1638886511909,"user_tz":0,"elapsed":27,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"1f66e961-f5dc-45d5-b3dd-c1d20d1188d4"},"source":["for sent in testdata[1:10]:\n","  print(sent)\n","  # break"],"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["{'category': 'neu', 'text': 'ครับ #phithanbkk', 'misp_tokens': [], 'tokenized': ['ครับ', ' ', '#phithanbkk'], 'segments': [[['ครับ', ' ', '#phithanbkk'], ['ครับ', ' ', '#phithanbkk']]]}\n","{'category': 'neg', 'text': 'การด่าไปเหมือนได้บรรเทาความเครียดเฉยๆ แต่บีทีเอส (รถไฟฟ้า) มันสำนึกมั้ย ก็ไม่อ่ะ 😕', 'misp_tokens': [{'corr': 'ไหม', 'misp': 'มั้ย', 'int': True, 's': 67, 't': 71}], 'tokenized': ['การ', 'ด่า', 'ไป', 'เหมือน', 'ได้', 'บรรเทา', 'ความ', 'เครียด', 'เฉย', 'ๆ', ' ', 'แต่', 'บีทีเอส', ' ', '(', 'รถ', 'ไฟฟ้า', ')', ' ', 'มัน', 'สำนึก', 'มั้ย', ' ', 'ก็', 'ไม่', 'อ่ะ', ' ', 'confused', ' ', 'face'], 'segments': [[['การ', 'ด่า', 'ไป', 'เหมือน', 'ได้', 'บรรเทา', 'ความ', 'เครียด', 'เฉย', 'ๆ', ' ', 'แต่', 'บีทีเอส', ' ', '(', 'รถ', 'ไฟฟ้า', ')', ' ', 'มัน', 'สำนึก'], ['การ', 'ด่า', 'ไป', 'เหมือน', 'ได้', 'บรรเทา', 'ความ', 'เครียด', 'เฉย', 'ๆ', ' ', 'แต่', 'บีทีเอส', ' ', '(', 'รถ', 'ไฟฟ้า', ')', ' ', 'มัน', 'สำนึก']], [['มั้ย'], ['ไหม']], [['ก็', 'ไม่', 'อ่ะ', ' ', 'confused', ' ', 'face'], ['ก็', 'ไม่', 'อ่ะ', ' ', 'confused', ' ', 'face']]]}\n","{'category': 'neu', 'text': 'Cf clarins 5 ขวด 2850', 'misp_tokens': [], 'tokenized': ['Cf', ' ', 'clarins', ' ', '5', ' ', 'ขวด', ' ', '2850'], 'segments': [[['Cf', ' ', 'clarins', ' ', '5', ' ', 'ขวด', ' ', '2850'], ['Cf', ' ', 'clarins', ' ', '5', ' ', 'ขวด', ' ', '2850']]]}\n","{'category': 'neu', 'text': 'ทานได้ค่ะ น้ำซุป MK ต้มมาจากหัวผักกาด ซีอิ้วขาว เกลือ แลน้ำตาลค่ะ', 'misp_tokens': [{'corr': 'และ', 'misp': 'แล', 'int': False, 's': 54, 't': 56}], 'tokenized': ['ทาน', 'ได้', 'ค่ะ', ' ', 'น้ำ', 'ซุป', ' ', 'MK', ' ', 'ต้ม', 'มา', 'จาก', 'หัว', 'ผักกาด', ' ', 'ซีอิ้วขาว', ' ', 'เกลือ', ' ', 'แล', 'น้ำตาล', 'ค่ะ'], 'segments': [[['ทาน', 'ได้', 'ค่ะ', ' ', 'น้ำ', 'ซุป', ' ', 'MK', ' ', 'ต้ม', 'มา', 'จาก', 'หัว', 'ผักกาด', ' ', 'ซีอิ้วขาว', ' ', 'เกลือ'], ['ทาน', 'ได้', 'ค่ะ', ' ', 'น้ำ', 'ซุป', ' ', 'MK', ' ', 'ต้ม', 'มา', 'จาก', 'หัว', 'ผักกาด', ' ', 'ซีอิ้วขาว', ' ', 'เกลือ']], [['แล'], ['และ']], [['น้ำตาล', 'ค่ะ'], ['น้ำตาล', 'ค่ะ']]]}\n","{'category': 'neu', 'text': 'เคล็ดลับที่ขาดไม่ได้ในการป้องกันผิวจากแสงแดด คือการทาครีมกันแดด สาวๆบ้างคนอาจจะคิดว่ามันไม่ใช่เรื่องสำคัญเท่าไหร่ แต่บอกเลยว่า ผิดมาก เพราะแสงแดดสมัยนี้แรงมาก และมีอนุภาพการทำลายผิวสูงมาก ถ้าไม่อยากให้ผิวเราถูกทำร้ายแบบซ้ำๆซาก ควรทาครีมกันแดดที่ดีมีคุณภาพอย่าง Eucerin Sun Age Repair Serum ปัจจุบันครีมกันแดดมีมากมายให้เราเลือกซื้อก็จริง แต่ก็อย่าลืมเลือกสิ่งที่ดีที่สุดให้ตัวเราอย่าง Eucerin Sun Acne oil Control ค่ะ', 'misp_tokens': [], 'tokenized': ['เคล็ด', 'ลับ', 'ที่', 'ขาด', 'ไม่', 'ได้', 'ใน', 'การ', 'ป้องกัน', 'ผิว', 'จาก', 'แสง', 'แดด', ' ', 'คือ', 'การ', 'ทา', 'ครีม', 'กัน', 'แดด', ' ', 'สาว', 'ๆ', 'บ้าง', 'คน', 'อาจ', 'จะ', 'คิด', 'ว่า', 'มัน', 'ไม่', 'ใช่', 'เรื่อง', 'สำคัญ', 'เท่า', 'ไหร่', ' ', 'แต่', 'บอก', 'เลย', 'ว่า', ' ', 'ผิด', 'มาก', ' ', 'เพราะ', 'แสง', 'แดด', 'สมัย', 'นี้', 'แรง', 'มาก', ' ', 'และ', 'มี', 'อนุภาพ', 'การ', 'ทำลาย', 'ผิว', 'สูง', 'มาก', ' ', 'ถ้า', 'ไม่', 'อยาก', 'ให้', 'ผิว', 'เรา', 'ถูก', 'ทำร้าย', 'แบบ', 'ซ้ำ', 'ๆ', 'ซาก', ' ', 'ควร', 'ทา', 'ครีม', 'กัน', 'แดด', 'ที่', 'ดี', 'มี', 'คุณภาพ', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Age', ' ', 'Repair', ' Serum', ' ', 'ปัจจุบัน', 'ครีม', 'กัน', 'แดด', 'มี', 'มากมาย', 'ให้', 'เรา', 'เลือก', 'ซื้อ', 'ก็', 'จริง', ' ', 'แต่', 'ก็', 'อย่า', 'ลืม', 'เลือก', 'สิ่ง', 'ที่', 'ดี', 'ที่สุด', 'ให้', 'ตัว', 'เรา', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Acne', ' ', 'oil', ' ', 'Control', ' ', 'ค่ะ'], 'segments': [[['เคล็ด', 'ลับ', 'ที่', 'ขาด', 'ไม่', 'ได้', 'ใน', 'การ', 'ป้องกัน', 'ผิว', 'จาก', 'แสง', 'แดด', ' ', 'คือ', 'การ', 'ทา', 'ครีม', 'กัน', 'แดด', ' ', 'สาว', 'ๆ', 'บ้าง', 'คน', 'อาจ', 'จะ', 'คิด', 'ว่า', 'มัน', 'ไม่', 'ใช่', 'เรื่อง', 'สำคัญ', 'เท่า', 'ไหร่', ' ', 'แต่', 'บอก', 'เลย', 'ว่า', ' ', 'ผิด', 'มาก', ' ', 'เพราะ', 'แสง', 'แดด', 'สมัย', 'นี้', 'แรง', 'มาก', ' ', 'และ', 'มี', 'อนุภาพ', 'การ', 'ทำลาย', 'ผิว', 'สูง', 'มาก', ' ', 'ถ้า', 'ไม่', 'อยาก', 'ให้', 'ผิว', 'เรา', 'ถูก', 'ทำร้าย', 'แบบ', 'ซ้ำ', 'ๆ', 'ซาก', ' ', 'ควร', 'ทา', 'ครีม', 'กัน', 'แดด', 'ที่', 'ดี', 'มี', 'คุณภาพ', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Age', ' ', 'Repair', ' Serum', ' ', 'ปัจจุบัน', 'ครีม', 'กัน', 'แดด', 'มี', 'มากมาย', 'ให้', 'เรา', 'เลือก', 'ซื้อ', 'ก็', 'จริง', ' ', 'แต่', 'ก็', 'อย่า', 'ลืม', 'เลือก', 'สิ่ง', 'ที่', 'ดี', 'ที่สุด', 'ให้', 'ตัว', 'เรา', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Acne', ' ', 'oil', ' ', 'Control', ' ', 'ค่ะ'], ['เคล็ด', 'ลับ', 'ที่', 'ขาด', 'ไม่', 'ได้', 'ใน', 'การ', 'ป้องกัน', 'ผิว', 'จาก', 'แสง', 'แดด', ' ', 'คือ', 'การ', 'ทา', 'ครีม', 'กัน', 'แดด', ' ', 'สาว', 'ๆ', 'บ้าง', 'คน', 'อาจ', 'จะ', 'คิด', 'ว่า', 'มัน', 'ไม่', 'ใช่', 'เรื่อง', 'สำคัญ', 'เท่า', 'ไหร่', ' ', 'แต่', 'บอก', 'เลย', 'ว่า', ' ', 'ผิด', 'มาก', ' ', 'เพราะ', 'แสง', 'แดด', 'สมัย', 'นี้', 'แรง', 'มาก', ' ', 'และ', 'มี', 'อนุภาพ', 'การ', 'ทำลาย', 'ผิว', 'สูง', 'มาก', ' ', 'ถ้า', 'ไม่', 'อยาก', 'ให้', 'ผิว', 'เรา', 'ถูก', 'ทำร้าย', 'แบบ', 'ซ้ำ', 'ๆ', 'ซาก', ' ', 'ควร', 'ทา', 'ครีม', 'กัน', 'แดด', 'ที่', 'ดี', 'มี', 'คุณภาพ', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Age', ' ', 'Repair', ' Serum', ' ', 'ปัจจุบัน', 'ครีม', 'กัน', 'แดด', 'มี', 'มากมาย', 'ให้', 'เรา', 'เลือก', 'ซื้อ', 'ก็', 'จริง', ' ', 'แต่', 'ก็', 'อย่า', 'ลืม', 'เลือก', 'สิ่ง', 'ที่', 'ดี', 'ที่สุด', 'ให้', 'ตัว', 'เรา', 'อย่าง', ' ', 'Eucerin', ' ', 'Sun', ' ', 'Acne', ' ', 'oil', ' ', 'Control', ' ', 'ค่ะ']]]}\n","{'category': 'neu', 'text': \"สำหรับสูตรของผม คือ Jack Daniel's หวานซ่อนเปรี้ยวชื่อ Jack Yellow Life ส่วนประกอบ Jack Daniel's 2 ชอท น้ำเก๊กฮวย น้ำมะนาว ใบมิ้นท์ เกลือเล็กน้อย วิธีปรุง นำ JackDaniel's มาเขย่ากับน้ำเก๊กฮวย เจือด้วยน้ำมะนาวบางๆ ตกแต่งด้วยใบมิ้น เสริฟด้วยแก้วที่ทาเกลือไว้ที่ปากแก้ว รสชาติที่จะได้คือหวาน หอม ซ่อนเปรี้ยว ด้วยคอนเซ็ปว่านี่แหละคือชีวิต จะหวานอย่างเดียวก็จะเลี่ยนไป จะเปรี้ยวเกินไปก็ไม่ใช่เรื่อง จึงควรจะมีทั้งเปรี้ยว ทั้งหวานคละเคล้ากันไปครับ\", 'misp_tokens': [], 'tokenized': ['สำหรับ', 'สูตร', 'ของ', 'ผม', ' ', 'คือ', ' ', 'Jack Daniel', \"'\", 's', ' ', 'หวาน', 'ซ่อน', 'เปรี้ยว', 'ชื่อ', ' ', 'Jack Yellow', ' ', 'Life', ' ', 'ส่วน', 'ประกอบ', ' ', 'Jack Daniel', \"'\", 's', ' ', '2', ' ', 'ชอท', ' ', 'น้ำ', 'เก๊กฮวย', ' ', 'น้ำ', 'มะนาว', ' ', 'ใบ', 'มิ้นท์', ' ', 'เกลือ', 'เล็กน้อย', ' ', 'วิธี', 'ปรุง', ' ', 'นำ', ' ', 'JackDaniel', \"'\", 's', ' ', 'มา', 'เขย่า', 'กับ', 'น้ำ', 'เก๊กฮวย', ' ', 'เจือ', 'ด้วย', 'น้ำ', 'มะนาว', 'บาง', 'ๆ', ' ', 'ตกแต่ง', 'ด้วย', 'ใบมิ้น', ' ', 'เสริฟ', 'ด้วย', 'แก้ว', 'ที่', 'ทาเกลือ', 'ไว้', 'ที่', 'ปากแก้ว', ' ', 'รสชาติ', 'ที่', 'จะ', 'ได้', 'คือ', 'หวาน', ' ', 'หอม', ' ', 'ซ่อน', 'เปรี้ยว', ' ', 'ด้วย', 'คอนเซ็ป', 'ว่า', 'นี่', 'แหละ', 'คือ', 'ชีวิต', ' ', 'จะ', 'หวาน', 'อย่าง', 'เดียว', 'ก็', 'จะ', 'เลี่ยน', 'ไป', ' ', 'จะ', 'เปรี้ยว', 'เกิน', 'ไป', 'ก็', 'ไม่', 'ใช่', 'เรื่อง', ' ', 'จึง', 'ควร', 'จะ', 'มี', 'ทั้ง', 'เปรี้ยว', ' ', 'ทั้งหวาน', 'คละเคล้า', 'กัน', 'ไป', 'ครับ'], 'segments': [[['สำหรับ', 'สูตร', 'ของ', 'ผม', ' ', 'คือ', ' ', 'Jack Daniel', \"'\", 's', ' ', 'หวาน', 'ซ่อน', 'เปรี้ยว', 'ชื่อ', ' ', 'Jack Yellow', ' ', 'Life', ' ', 'ส่วน', 'ประกอบ', ' ', 'Jack Daniel', \"'\", 's', ' ', '2', ' ', 'ชอท', ' ', 'น้ำ', 'เก๊กฮวย', ' ', 'น้ำ', 'มะนาว', ' ', 'ใบ', 'มิ้นท์', ' ', 'เกลือ', 'เล็กน้อย', ' ', 'วิธี', 'ปรุง', ' ', 'นำ', ' ', 'JackDaniel', \"'\", 's', ' ', 'มา', 'เขย่า', 'กับ', 'น้ำ', 'เก๊กฮวย', ' ', 'เจือ', 'ด้วย', 'น้ำ', 'มะนาว', 'บาง', 'ๆ', ' ', 'ตกแต่ง', 'ด้วย', 'ใบมิ้น', ' ', 'เสริฟ', 'ด้วย', 'แก้ว', 'ที่', 'ทาเกลือ', 'ไว้', 'ที่', 'ปากแก้ว', ' ', 'รสชาติ', 'ที่', 'จะ', 'ได้', 'คือ', 'หวาน', ' ', 'หอม', ' ', 'ซ่อน', 'เปรี้ยว', ' ', 'ด้วย', 'คอนเซ็ป', 'ว่า', 'นี่', 'แหละ', 'คือ', 'ชีวิต', ' ', 'จะ', 'หวาน', 'อย่าง', 'เดียว', 'ก็', 'จะ', 'เลี่ยน', 'ไป', ' ', 'จะ', 'เปรี้ยว', 'เกิน', 'ไป', 'ก็', 'ไม่', 'ใช่', 'เรื่อง', ' ', 'จึง', 'ควร', 'จะ', 'มี', 'ทั้ง', 'เปรี้ยว', ' ', 'ทั้งหวาน', 'คละเคล้า', 'กัน', 'ไป', 'ครับ'], ['สำหรับ', 'สูตร', 'ของ', 'ผม', ' ', 'คือ', ' ', 'Jack Daniel', \"'\", 's', ' ', 'หวาน', 'ซ่อน', 'เปรี้ยว', 'ชื่อ', ' ', 'Jack Yellow', ' ', 'Life', ' ', 'ส่วน', 'ประกอบ', ' ', 'Jack Daniel', \"'\", 's', ' ', '2', ' ', 'ชอท', ' ', 'น้ำ', 'เก๊กฮวย', ' ', 'น้ำ', 'มะนาว', ' ', 'ใบ', 'มิ้นท์', ' ', 'เกลือ', 'เล็กน้อย', ' ', 'วิธี', 'ปรุง', ' ', 'นำ', ' ', 'JackDaniel', \"'\", 's', ' ', 'มา', 'เขย่า', 'กับ', 'น้ำ', 'เก๊กฮวย', ' ', 'เจือ', 'ด้วย', 'น้ำ', 'มะนาว', 'บาง', 'ๆ', ' ', 'ตกแต่ง', 'ด้วย', 'ใบมิ้น', ' ', 'เสริฟ', 'ด้วย', 'แก้ว', 'ที่', 'ทาเกลือ', 'ไว้', 'ที่', 'ปากแก้ว', ' ', 'รสชาติ', 'ที่', 'จะ', 'ได้', 'คือ', 'หวาน', ' ', 'หอม', ' ', 'ซ่อน', 'เปรี้ยว', ' ', 'ด้วย', 'คอนเซ็ป', 'ว่า', 'นี่', 'แหละ', 'คือ', 'ชีวิต', ' ', 'จะ', 'หวาน', 'อย่าง', 'เดียว', 'ก็', 'จะ', 'เลี่ยน', 'ไป', ' ', 'จะ', 'เปรี้ยว', 'เกิน', 'ไป', 'ก็', 'ไม่', 'ใช่', 'เรื่อง', ' ', 'จึง', 'ควร', 'จะ', 'มี', 'ทั้ง', 'เปรี้ยว', ' ', 'ทั้งหวาน', 'คละเคล้า', 'กัน', 'ไป', 'ครับ']]]}\n","{'category': 'neg', 'text': 'เจ้ว่าการ์นิเย่แอบแรงนิสหน่อย เคยใช้โยเกิร์ตไม๊ พรุ้งนี้ลองพอกหน้าดูทำให้หน้าสบายขึ้น นุ่มขึ้น หายไวไวน๊าาาา', 'misp_tokens': [{'corr': 'นะ', 'misp': 'น๊าาาา', 'int': True, 's': 102, 't': 108}, {'corr': 'นิดหน่อย', 'misp': 'นิสหน่อย', 'int': True, 's': 21, 't': 29}, {'corr': 'ไหม', 'misp': 'ไม๊', 'int': True, 's': 44, 't': 47}], 'tokenized': ['เจ้ว่า', 'การ์นิเย่', 'แอบ', 'แรง', 'นิสหน่อย', ' ', 'เคย', 'ใช้', 'โยเกิร์ต', 'ไม๊', ' ', 'พรุ้ง', 'นี้', 'ลอง', 'พอก', 'หน้า', 'ดู', 'ทำ', 'ให้', 'หน้า', 'สบาย', 'ขึ้น', ' ', 'นุ่ม', 'ขึ้น', ' ', 'หาย', 'ไว', 'ไวน๊าาาา'], 'segments': [[['เจ้ว่า', 'การ์นิเย่', 'แอบ', 'แรง'], ['เจ้ว่า', 'การ์นิเย่', 'แอบ', 'แรง']], [['นิสหน่อย'], ['นิดหน่อย']], [['เคย', 'ใช้', 'โยเกิร์ต'], ['เคย', 'ใช้', 'โยเกิร์ต']], [['ไม๊'], ['ไหม']], [['พรุ้ง', 'นี้', 'ลอง', 'พอก', 'หน้า', 'ดู', 'ทำ', 'ให้', 'หน้า', 'สบาย', 'ขึ้น', ' ', 'นุ่ม', 'ขึ้น', ' ', 'หาย', 'ไวไว'], ['พรุ้ง', 'นี้', 'ลอง', 'พอก', 'หน้า', 'ดู', 'ทำ', 'ให้', 'หน้า', 'สบาย', 'ขึ้น', ' ', 'นุ่ม', 'ขึ้น', ' ', 'หาย', 'ไวไว']], [['น๊าาาา'], ['นะ']], [[], []]]}\n","{'category': 'neu', 'text': 'เอๅจริงๆถ้ๅมันเปิดให้ใช้และถูกกฎหมๅยจริวคงกลัวจะเก็บภๅษียังไงรึป่ๅว', 'misp_tokens': [{'corr': 'เอา', 'misp': 'เอๅ', 'int': False, 's': 0, 't': 3}, {'corr': 'ถ้า', 'misp': 'ถ้ๅ', 'int': False, 's': 8, 't': 11}, {'corr': 'กฎหมาย', 'misp': 'กฎหมๅย', 'int': False, 's': 30, 't': 36}, {'corr': 'จริง', 'misp': 'จริว', 'int': False, 's': 36, 't': 40}, {'corr': 'ฤๅษี', 'misp': 'ภๅษี', 'int': False, 's': 52, 't': 56}, {'corr': 'เปล่า', 'misp': 'ป่ๅว', 'int': False, 's': 63, 't': 67}], 'tokenized': ['เอๅจริง', 'ๆ', 'ถ้ๅ', 'มัน', 'เปิด', 'ให้', 'ใช้', 'และ', 'ถูก', 'กฎหมๅย', 'จริว', 'คง', 'กลัว', 'จะ', 'เก็บ', 'ภๅษี', 'ยัง', 'ไง', 'รึป่ๅว'], 'segments': [[[], []], [['เอๅ'], ['เอา']], [['จริง', 'ๆ'], ['จริง', 'ๆ']], [['ถ้ๅ'], ['ถ้า']], [['มัน', 'เปิด', 'ให้', 'ใช้', 'และ', 'ถูก'], ['มัน', 'เปิด', 'ให้', 'ใช้', 'และ', 'ถูก']], [['กฎหมๅย'], ['กฎหมาย']], [[], []], [['จริว'], ['จริง']], [['คง', 'กลัว', 'จะ', 'เก็บ'], ['คง', 'กลัว', 'จะ', 'เก็บ']], [['ภๅษี'], ['ฤๅษี']], [['ยัง', 'ไง', 'รึ'], ['ยัง', 'ไง', 'รึ']], [['ป่ๅว'], ['เปล่า']], [[], []]]}\n","{'category': 'neg', 'text': 'อิผ้าอนามัยเหิ้ย ติดทุกอย่าง ยกเว้นกางเกงใน', 'misp_tokens': [{'corr': 'อี', 'misp': 'อิ', 'int': True, 's': 0, 't': 2}, {'corr': 'เหี้ย', 'misp': 'เหิ้ย', 'int': False, 's': 11, 't': 16}], 'tokenized': ['อิผ้า', 'อนามัย', 'เหิ้ย', ' ', 'ติด', 'ทุก', 'อย่าง', ' ', 'ยกเว้น', 'กางเกง', 'ใน'], 'segments': [[[], []], [['อิ'], ['อี']], [['ผ้า', 'อนามัย'], ['ผ้า', 'อนามัย']], [['เหิ้ย'], ['เหี้ย']], [['ติด', 'ทุก', 'อย่าง', ' ', 'ยกเว้น', 'กางเกง', 'ใน'], ['ติด', 'ทุก', 'อย่าง', ' ', 'ยกเว้น', 'กางเกง', 'ใน']]]}\n"]}]},{"cell_type":"code","metadata":{"id":"NRWpDhQaitQc","executionInfo":{"status":"ok","timestamp":1638886511909,"user_tz":0,"elapsed":12,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"yceBl7D2jGyv","executionInfo":{"status":"ok","timestamp":1638886511910,"user_tz":0,"elapsed":12,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLTGbmXyf0gp","executionInfo":{"status":"ok","timestamp":1638886511910,"user_tz":0,"elapsed":11,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["import glob\n","from torch.utils.data import Dataset as TorchDataset\n","from datasets import Dataset\n","# from thai2transformers.datasets import SequenceClassificationDataset\n","\n","class SequenceClassificationDataset(TorchDataset):\n","    def __init__(\n","        self,\n","        tokenizer,\n","        data_dir,\n","        task=Task.MULTICLASS_CLS,\n","        max_length=128,\n","        ext=\".csv\",\n","        bs=10000,\n","        preprocessor=None,\n","        input_ids=[],\n","        misp_ids=[],\n","        attention_masks=[],\n","        labels=[],\n","        label_encoder=None\n","    ):\n","        self.max_length = max_length\n","        self.tokenizer = tokenizer\n","        self.bs = bs\n","        self.preprocessor = preprocessor\n","        self.input_ids = input_ids\n","        self.misp_ids = misp_ids\n","        self.attention_masks = attention_masks\n","        self.labels = labels\n","        self.task = task\n","        self.label_encoder = label_encoder\n","        # self._build()\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, i):\n","        return {\n","            \"input_ids\": torch.tensor(self.input_ids[i], dtype=torch.long),\n","            \"misp_ids\": torch.tensor(self.misp_ids[i], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(self.attention_masks[i], dtype=torch.long),\n","            \"label\": torch.tensor(self.labels[i], dtype=torch.long),\n","        }\n"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Zxb64qIt674","executionInfo":{"status":"ok","timestamp":1638886511911,"user_tz":0,"elapsed":11,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxC53Pw1zQ-V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638886520515,"user_tz":0,"elapsed":8614,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"0b429807-c94d-4586-d5b5-cc487f03aa0c"},"source":["from tqdm import tqdm \n","import itertools\n","\n","LABELS = {\n","    \"neg\": 2,\n","    \"neu\": 1,\n","    \"pos\": 0,\n","    \"q\": 1 # used to be 3\n","}\n","\n","class CustomLabelEncoder():\n","    def __init__(self):\n","        pass\n","\n","    def transform(self, labels):\n","        return [LABELS[l] for l in labels]\n","    \n","\n","def remove_starting_marker(t, unk):\n","    if len(t) > 0:\n","      if t[0]=='▁':\n","        t = t[1:]\n","      elif t[0].startswith('▁'):        \n","        if tokenizer.convert_tokens_to_ids([t[0][1:]])[0] != unk:\n","          t[0] = t[0][1:]\n","    return t\n","\n","def preprocessing(d, preSegmented=False, mode=None):\n","    max_length = 400\n","    custom_label_encoder = CustomLabelEncoder()\n","    labels = get_dict_val(d, \"category\")\n","\n","    labels = custom_label_encoder.transform(labels)\n","\n","    input_ids = []\n","    misp_ids = []\n","    attention_masks = []\n","    unk = tokenizer.convert_tokens_to_ids([\"<unk>\"])[0]\n","\n","    sents = []\n","    if not preSegmented:\n","      texts = get_dict_val(d, \"tokenized\")\n","      for tokens in tqdm(texts):\n","        tokens = [(t, t) for t in tokens]\n","        sents.append(tokens)\n","        \n","    else:\n","      texts = get_dict_val(d, \"segments\")\n","      for segments in tqdm(texts):\n","        s = [list(zip(seg[0], seg[1])) for seg in segments]\n","        tokens = list(itertools.chain(*s))\n","        sents.append(tokens)\n","\n","    for tokens in sents:\n","      # if mode is None, ignore corr\n","      misptokens = [t[0] for t in tokens]\n","      corrtokens = [t[0] for t in tokens]\n","      \n","      if mode==\"corr\":\n","        misptokens = [t[1] for t in tokens]\n","        corrtokens = [t[1] for t in tokens]\n","      elif mode==\"mae\":\n","        misptokens = [t[0] for t in tokens]\n","        corrtokens = [t[1] for t in tokens]\n","      \n","      \n","      midx = tokenizer.convert_tokens_to_ids(misptokens)\n","      cidx = tokenizer.convert_tokens_to_ids(corrtokens)\n","      assert(len(midx)==len(cidx))\n","\n","      newmisptokens = []\n","      newcorrtokens = []\n","      for i in range(len(midx)):\n","          if midx[i]==unk:\n","              t = tokenizer.tokenize(misptokens[i])\n","              t = remove_starting_marker(t, unk)\n","\n","              if misptokens[i]==corrtokens[i]:\n","                newmisptokens += t\n","                newcorrtokens += t\n","              else:\n","                newmisptokens += t\n","                tx = tokenizer.tokenize(corrtokens[i])\n","                tx = remove_starting_marker(tx, unk)\n","\n","                if len(tx) > 0:\n","                  newcorrtokens += [tx[0] for j in range(len(t))]\n","                else:\n","                  newcorrtokens += t\n","          else:\n","              newmisptokens.append(misptokens[i])\n","              newcorrtokens.append(corrtokens[i])\n","\n","      assert(len(newmisptokens)==len(newcorrtokens))\n","              \n","      # words = newwords\n","      newmisptokens = ['<s>'] + newmisptokens[0:max_length-2] + ['</s>']\n","      newcorrtokens = ['<s>'] + newcorrtokens[0:max_length-2] + ['</s>']\n","    \n","      midx = tokenizer.convert_tokens_to_ids(newmisptokens)\n","      cidx = tokenizer.convert_tokens_to_ids(newcorrtokens)\n","\n","      mask = [1 for i in midx]\n","        \n","      input_ids.append(midx)\n","      misp_ids.append(cidx)\n","      attention_masks.append(mask)\n","\n","    #   if len(input_ids) > 10:\n","    #     break\n","    \n","    # labels = labels[0:10]\n","\n","    return SequenceClassificationDataset(\n","        tokenizer=tokenizer,\n","        data_dir=None,\n","        max_length=max_length,\n","        input_ids=input_ids,\n","        misp_ids=misp_ids,\n","        attention_masks=attention_masks,\n","        labels=labels,\n","        task=task\n","    )\n","\n","raw_datasets = {\n","    \"train\": traindata,\n","    \"validation\": validdata,\n","    \"test\": allTestdata,\n","    \"test-corr\": corrTestdata,\n","    \"test-misp\": mispTestdata,\n","    \"test-mae\": mispTestdata,\n","    \"test-all-mae\": allTestdata,\n","}\n","\n","dataset_split = {}\n","for split_name in raw_datasets:\n","    print(\"Tokenizing:\", split_name)\n","    d = pd.DataFrame(raw_datasets[split_name])\n","    d = Dataset.from_pandas(d)\n","    preSegmented = split_name.startswith(\"test\") or split_name.startswith(\"train\")\n","    mode = None\n","    if \"corr\" in split_name:\n","      mode = \"corr\"\n","    elif \"misp\" in split_name:\n","      mode = \"misp\"\n","    elif \"mae\" in split_name:\n","      mode = \"mae\"\n","\n","    dataset_split[split_name] = preprocessing(d, preSegmented=preSegmented, mode=mode)\n","    # break"],"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing: train\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3000/3000 [00:00<00:00, 125519.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2404/2404 [00:00<00:00, 194298.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2671/2671 [00:00<00:00, 138913.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-corr\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 880/880 [00:00<00:00, 90281.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-misp\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 880/880 [00:00<00:00, 81311.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-mae\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 880/880 [00:00<00:00, 61892.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Tokenizing: test-all-mae\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2671/2671 [00:00<00:00, 108905.36it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"zTo1InxM7QlH","executionInfo":{"status":"ok","timestamp":1638886520517,"user_tz":0,"elapsed":23,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVpOylu-CrqE"},"source":["## Custom Classes for Our Experiments"]},{"cell_type":"code","metadata":{"id":"wHqjltTIEdzZ","executionInfo":{"status":"ok","timestamp":1638886520517,"user_tz":0,"elapsed":21,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["from transformers.modeling_roberta import RobertaEmbeddings"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Q5iZsOgEd2P","executionInfo":{"status":"ok","timestamp":1638886520518,"user_tz":0,"elapsed":21,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["from torch import nn\n","\n","class CustomRobertaEmbeddings(RobertaEmbeddings):\n","\n","    def __init__(self, ref, config):\n","        super().__init__(config)\n","        self.word_embeddings = ref.word_embeddings\n","\n","    def forward(self, input_ids, misp_ids, token_type_ids=None, position_ids=None, inputs_embeds=None):\n","\n","        input_shape = input_ids.size()\n","        seq_length = input_shape[1]\n","\n","        inputs_embeds = self.word_embeddings(input_ids)\n","        misp_embeds = self.word_embeddings(misp_ids)\n","\n","        embeddings = ((inputs_embeds + misp_embeds)*0.5)\n","        return embeddings\n"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL_oBhOdHTjt","executionInfo":{"status":"ok","timestamp":1638886520518,"user_tz":0,"elapsed":20,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["from transformers.modeling_roberta import RobertaModel"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ew6s3Y1f1vCa","executionInfo":{"status":"ok","timestamp":1638886520519,"user_tz":0,"elapsed":20,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["from transformers.modeling_camembert import CamembertForSequenceClassification\n","from transformers.modeling_roberta import RobertaForSequenceClassification\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","\n","class CustomSequenceClassification(CamembertForSequenceClassification):\n","    authorized_missing_keys = [r\"position_ids\"]\n","\n","    def __init__(self, config, refmodel=None):\n","        super().__init__(config)\n","        if refmodel is not None:\n","          config = refmodel.config\n","          # self.refmodel = refmodel\n","          self.num_labels = config.num_labels\n","\n","          self.roberta = refmodel.roberta\n","          self.classifier = refmodel.classifier\n","\n","          self.baseEmb = refmodel.roberta.embeddings\n","          self.newEmb = CustomRobertaEmbeddings(self.baseEmb, config)\n","\n","    def forward(self, *args, **kwargs):\n","        # del kwargs[\"misp_ids\"]\n","        # return self.refmodel(**kwargs)\n","\n","        return_dict = self.config.use_return_dict\n","\n","        inputs_embeds = self.newEmb(kwargs[\"input_ids\"], kwargs[\"misp_ids\"])\n","        \n","        # doc: https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel.forward\n","        outputs = self.roberta(\n","            input_ids=None,\n","            attention_mask=kwargs[\"attention_mask\"],\n","            token_type_ids=None,\n","            position_ids=None,\n","            head_mask=None,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        labels = kwargs[\"labels\"]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"lznRUPu1YJSA","executionInfo":{"status":"ok","timestamp":1638886520519,"user_tz":0,"elapsed":19,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# cusmodel = CustomSequenceClassification(model.config, model)\n","# trainer, training_args = init_trainer(task=task,\n","#                             model=cusmodel,\n","#                             train_dataset=dataset_split['train'],\n","#                             val_dataset=dataset_split['validation'] if 'validation' in DATASET_METADATA[args.dataset_name]['split_names'] else None,\n","#                             warmup_steps=warmup_steps,\n","#                             args=args,\n","#                             data_collator=data_collator)\n","\n","# p, label_ids, result = trainer.predict(test_dataset=dataset_split['test'])\n","\n","# print(f'Evaluation on test set (dataset: {args.dataset_name})')    \n","\n","# for key, value in result.items():\n","#     print(f'{key} : {value:.4f}')\n","\n","# # Evaluation on test set (dataset: wisesight_sentiment)\n","# # eval_loss : 1.0284\n","# # eval_accuracy : 0.3000\n","# # eval_f1_micro : 0.3000\n","# # eval_precision_micro : 0.3000\n","# # eval_recall_micro : 0.3000\n","# # eval_f1_macro : 0.1667\n","# # eval_precision_macro : 0.1667\n","# # eval_recall_macro : 0.1667\n","# # eval_nb_samples : 10.0000"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDP2AEpDCqXh","executionInfo":{"status":"ok","timestamp":1638886520519,"user_tz":0,"elapsed":19,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["from dataclasses import dataclass\n","from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n","from transformers import PreTrainedTokenizerBase\n","\n","@dataclass\n","class CustomDataCollatorWithPadding:\n","  tokenizer: PreTrainedTokenizerBase\n","  padding: Union[bool, str] = True\n","  max_length: Optional[int] = None\n","  pad_to_multiple_of: Optional[int] = None\n","  return_tensors: str = \"pt\"\n","\n","  def __call__(self, features):\n","    _tmpfeat = []\n","    for f in features:\n","      _tmpfeat.append({\n","          \"input_ids\": f[\"input_ids\"],\n","          \"attention_mask\": f[\"attention_mask\"],\n","          \"label\": f[\"label\"],\n","      })\n","\n","    batch = self.tokenizer.pad(\n","        _tmpfeat,\n","        padding=self.padding,\n","        max_length=self.max_length,\n","        pad_to_multiple_of=self.pad_to_multiple_of,\n","        # return_tensors=self.return_tensors,\n","    )\n","\n","    _tmpfeat = []\n","    for f in features:\n","      _tmpfeat.append({\n","          \"input_ids\": f[\"misp_ids\"],\n","          \"attention_mask\": f[\"attention_mask\"],\n","          \"label\": f[\"label\"],\n","      })\n","      \n","    mispbatch = self.tokenizer.pad(\n","        _tmpfeat,\n","        padding=self.padding,\n","        max_length=self.max_length,\n","        pad_to_multiple_of=self.pad_to_multiple_of,\n","        # return_tensors=self.return_tensors,\n","    )\n","\n","    # print(mispbatch[\"input_ids\"])\n","    # print(tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0]))\n","    # print(tokenizer.convert_ids_to_tokens(mispbatch[\"input_ids\"][0]))\n","    batch[\"misp_ids\"] = mispbatch[\"input_ids\"]\n","    assert(batch[\"misp_ids\"].shape==batch[\"input_ids\"].shape)\n","    # assert()\n","    if \"label\" in batch:\n","        batch[\"labels\"] = batch[\"label\"]\n","        del batch[\"label\"]\n","    if \"label_ids\" in batch:\n","        batch[\"labels\"] = batch[\"label_ids\"]\n","        del batch[\"label_ids\"]\n","    return batch"],"execution_count":74,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_D7scYkC0Jg"},"source":["# Model Training "]},{"cell_type":"code","metadata":{"id":"Dx6AsnFmeQlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638886523893,"user_tz":0,"elapsed":3392,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"23e3316b-2c26-4e2b-ea82-7bd4147e7ad0"},"source":["warmup_steps = math.ceil(len(dataset_split['train']) / args.batch_size * args.warmup_ratio * args.num_train_epochs)\n","\n","print(f'\\n[INFO] Number of train examples = {len(raw_datasets[\"train\"])}')\n","print(f'[INFO] Number of batches per epoch (training set) = {math.ceil(len(dataset_split[\"train\"]) / args.batch_size)}')\n","\n","print(f'[INFO] Warmup ratio = {args.warmup_ratio}')\n","print(f'[INFO] Warmup steps = {warmup_steps}')\n","print(f'[INFO] Learning rate: {args.learning_rate}')\n","print(f'[INFO] Logging steps: {args.logging_steps}')\n","print(f'[INFO] FP16 training: {args.fp16}\\n')\n","\n","# if 'validation' in DATASET_METADATA[args.dataset_name]['split_names']:\n","print(f'[INFO] Number of validation examples = {len(raw_datasets[\"validation\"])}')\n","print(f'[INFO] Number of batches per epoch (validation set) = {math.ceil(len(dataset_split[\"validation\"]))}')\n","\n","data_collator = CustomDataCollatorWithPadding(tokenizer,\n","                                        padding=True,\n","                                        pad_to_multiple_of=8 if args.fp16 else None)\n","\n","cusmodel = CustomSequenceClassification(model.config, model)\n","trainer, training_args = init_trainer(task=task,\n","                            model=cusmodel,\n","                            train_dataset=dataset_split['train'],\n","                            val_dataset=dataset_split['validation'] if 'validation' in DATASET_METADATA[args.dataset_name]['split_names'] else None,\n","                            warmup_steps=warmup_steps,\n","                            args=args,\n","                            data_collator=data_collator)\n","\n","print('[INFO] TrainingArguments:')\n","print(training_args)\n","print('\\n')"],"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[INFO] Number of train examples = 3000\n","[INFO] Number of batches per epoch (training set) = 375\n","[INFO] Warmup ratio = 0.1\n","[INFO] Warmup steps = 750\n","[INFO] Learning rate: 1e-05\n","[INFO] Logging steps: 10\n","[INFO] FP16 training: False\n","\n","[INFO] Number of validation examples = 2404\n","[INFO] Number of batches per epoch (validation set) = 2404\n","[INFO] TrainingArguments:\n","TrainingArguments(output_dir='Models/WangchanBERTa-exp3/Outputs/', overwrite_output_dir=True, do_train=False, do_eval=None, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=20, max_steps=-1, warmup_steps=750, logging_dir='Models/WangchanBERTa-exp3/Logs/', logging_first_step=False, logging_steps=10, save_steps=500, save_total_limit=None, no_cuda=False, seed=0, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='exp1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='f1_micro', greater_is_better=True)\n","\n","\n"]}]},{"cell_type":"code","metadata":{"id":"cefiChazjOmO","executionInfo":{"status":"ok","timestamp":1638891589747,"user_tz":0,"elapsed":358,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":834},"id":"O8TTQ7VmQGhS","executionInfo":{"status":"ok","timestamp":1638890306965,"user_tz":0,"elapsed":3783100,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"641650a4-794f-40bc-ad36-3d8773b07520"},"source":["print('\\nBegin model finetuning.')\n","trainer.train()\n","print('Done.\\n')"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Begin model finetuning.\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7500/7500 1:03:02, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Micro</th>\n","      <th>Precision Micro</th>\n","      <th>Recall Micro</th>\n","      <th>F1 Macro</th>\n","      <th>Precision Macro</th>\n","      <th>Recall Macro</th>\n","      <th>Nb Samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.006195</td>\n","      <td>0.979337</td>\n","      <td>0.549085</td>\n","      <td>0.549085</td>\n","      <td>0.549085</td>\n","      <td>0.549085</td>\n","      <td>0.500581</td>\n","      <td>0.498849</td>\n","      <td>0.507735</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.996619</td>\n","      <td>0.993022</td>\n","      <td>0.486273</td>\n","      <td>0.486273</td>\n","      <td>0.486273</td>\n","      <td>0.486273</td>\n","      <td>0.488014</td>\n","      <td>0.559563</td>\n","      <td>0.578189</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.831433</td>\n","      <td>0.812681</td>\n","      <td>0.616473</td>\n","      <td>0.616473</td>\n","      <td>0.616473</td>\n","      <td>0.616473</td>\n","      <td>0.593651</td>\n","      <td>0.590317</td>\n","      <td>0.630628</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.678418</td>\n","      <td>0.791647</td>\n","      <td>0.636855</td>\n","      <td>0.636855</td>\n","      <td>0.636855</td>\n","      <td>0.636855</td>\n","      <td>0.623863</td>\n","      <td>0.624071</td>\n","      <td>0.669873</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.740564</td>\n","      <td>0.771341</td>\n","      <td>0.655574</td>\n","      <td>0.655574</td>\n","      <td>0.655574</td>\n","      <td>0.655574</td>\n","      <td>0.640936</td>\n","      <td>0.637492</td>\n","      <td>0.678798</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.674109</td>\n","      <td>0.800691</td>\n","      <td>0.630200</td>\n","      <td>0.630200</td>\n","      <td>0.630200</td>\n","      <td>0.630200</td>\n","      <td>0.618406</td>\n","      <td>0.622231</td>\n","      <td>0.654009</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.612866</td>\n","      <td>0.818938</td>\n","      <td>0.650166</td>\n","      <td>0.650166</td>\n","      <td>0.650166</td>\n","      <td>0.650166</td>\n","      <td>0.613565</td>\n","      <td>0.615853</td>\n","      <td>0.618261</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.599194</td>\n","      <td>0.898114</td>\n","      <td>0.633527</td>\n","      <td>0.633527</td>\n","      <td>0.633527</td>\n","      <td>0.633527</td>\n","      <td>0.627215</td>\n","      <td>0.637800</td>\n","      <td>0.671117</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.479712</td>\n","      <td>0.963338</td>\n","      <td>0.618968</td>\n","      <td>0.618968</td>\n","      <td>0.618968</td>\n","      <td>0.618968</td>\n","      <td>0.618208</td>\n","      <td>0.636692</td>\n","      <td>0.673903</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.401245</td>\n","      <td>0.918568</td>\n","      <td>0.651414</td>\n","      <td>0.651414</td>\n","      <td>0.651414</td>\n","      <td>0.651414</td>\n","      <td>0.629168</td>\n","      <td>0.623657</td>\n","      <td>0.653853</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.292285</td>\n","      <td>0.938173</td>\n","      <td>0.662646</td>\n","      <td>0.662646</td>\n","      <td>0.662646</td>\n","      <td>0.662646</td>\n","      <td>0.635563</td>\n","      <td>0.628224</td>\n","      <td>0.655095</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.415771</td>\n","      <td>0.977852</td>\n","      <td>0.650582</td>\n","      <td>0.650582</td>\n","      <td>0.650582</td>\n","      <td>0.650582</td>\n","      <td>0.622257</td>\n","      <td>0.613590</td>\n","      <td>0.642805</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.255029</td>\n","      <td>1.194755</td>\n","      <td>0.613561</td>\n","      <td>0.613561</td>\n","      <td>0.613561</td>\n","      <td>0.613561</td>\n","      <td>0.610040</td>\n","      <td>0.620597</td>\n","      <td>0.664248</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.279639</td>\n","      <td>1.188536</td>\n","      <td>0.627288</td>\n","      <td>0.627288</td>\n","      <td>0.627288</td>\n","      <td>0.627288</td>\n","      <td>0.618543</td>\n","      <td>0.620546</td>\n","      <td>0.670036</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.201025</td>\n","      <td>1.150573</td>\n","      <td>0.645175</td>\n","      <td>0.645175</td>\n","      <td>0.645175</td>\n","      <td>0.645175</td>\n","      <td>0.630392</td>\n","      <td>0.627328</td>\n","      <td>0.667423</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.226221</td>\n","      <td>1.241175</td>\n","      <td>0.637687</td>\n","      <td>0.637687</td>\n","      <td>0.637687</td>\n","      <td>0.637687</td>\n","      <td>0.629692</td>\n","      <td>0.634988</td>\n","      <td>0.674147</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.331836</td>\n","      <td>1.258286</td>\n","      <td>0.637687</td>\n","      <td>0.637687</td>\n","      <td>0.637687</td>\n","      <td>0.637687</td>\n","      <td>0.627442</td>\n","      <td>0.629196</td>\n","      <td>0.669815</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.269873</td>\n","      <td>1.267083</td>\n","      <td>0.641015</td>\n","      <td>0.641015</td>\n","      <td>0.641015</td>\n","      <td>0.641015</td>\n","      <td>0.627750</td>\n","      <td>0.626661</td>\n","      <td>0.666966</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.186328</td>\n","      <td>1.289404</td>\n","      <td>0.643927</td>\n","      <td>0.643927</td>\n","      <td>0.643927</td>\n","      <td>0.643927</td>\n","      <td>0.630512</td>\n","      <td>0.627970</td>\n","      <td>0.668933</td>\n","      <td>2404</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.311914</td>\n","      <td>1.302455</td>\n","      <td>0.638103</td>\n","      <td>0.638103</td>\n","      <td>0.638103</td>\n","      <td>0.638103</td>\n","      <td>0.627014</td>\n","      <td>0.628168</td>\n","      <td>0.667908</td>\n","      <td>2404</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at Models/WangchanBERTa-exp3/Outputs/checkpoint-4125 were not used when initializing CustomSequenceClassification: ['baseEmb.position_ids', 'baseEmb.word_embeddings.weight', 'baseEmb.position_embeddings.weight', 'baseEmb.token_type_embeddings.weight', 'baseEmb.LayerNorm.weight', 'baseEmb.LayerNorm.bias', 'newEmb.position_ids', 'newEmb.word_embeddings.weight', 'newEmb.position_embeddings.weight', 'newEmb.token_type_embeddings.weight', 'newEmb.LayerNorm.weight', 'newEmb.LayerNorm.bias']\n","- This IS expected if you are initializing CustomSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Done.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"K63mJONBgbiB","executionInfo":{"status":"ok","timestamp":1638890321598,"user_tz":0,"elapsed":14668,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["trainer.save_model(f\"{DIR}/fine-tune-Exp1\")"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIwrxhH2cLsR"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"lRczslIwcNmh","executionInfo":{"status":"ok","timestamp":1638890321600,"user_tz":0,"elapsed":14,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-iWxTbYZKh4","executionInfo":{"status":"ok","timestamp":1638890325017,"user_tz":0,"elapsed":3428,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["# assert(torch.equal(model.classifier.dense.weight, trainer.model.classifier.dense.weight))\n","trainer.model = CustomSequenceClassification(model.config, model)"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjcp02iSgYgC","executionInfo":{"status":"ok","timestamp":1638890325018,"user_tz":0,"elapsed":20,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":["trainer.model.eval();"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lAOts0QgwKba","executionInfo":{"status":"ok","timestamp":1638890482096,"user_tz":0,"elapsed":157095,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}},"outputId":"b3f8a04f-069b-4f01-ea24-a0e70a69ab87"},"source":["\n","for split_name in dataset_split:\n","  if split_name.startswith(\"train\"):\n","    continue\n","\n","  p, label_ids, result = trainer.predict(test_dataset=dataset_split[split_name])\n","  print(f'Evaluation on {split_name}:')    \n","\n","  for key, value in result.items():\n","      print(f'{key} : {value:.4f}')\n","  \n","  print(\"*\"*40)\n","  print()"],"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='1299' max='301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [301/301 02:37]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation on validation:\n","eval_loss : 1.3025\n","eval_accuracy : 0.6381\n","eval_f1_micro : 0.6381\n","eval_precision_micro : 0.6381\n","eval_recall_micro : 0.6381\n","eval_f1_macro : 0.6270\n","eval_precision_macro : 0.6282\n","eval_recall_macro : 0.6679\n","eval_nb_samples : 2404.0000\n","****************************************\n","\n","Evaluation on test:\n","eval_loss : 1.2807\n","eval_accuracy : 0.6451\n","eval_f1_micro : 0.6451\n","eval_precision_micro : 0.6451\n","eval_recall_micro : 0.6451\n","eval_f1_macro : 0.6340\n","eval_precision_macro : 0.6330\n","eval_recall_macro : 0.6776\n","eval_nb_samples : 2671.0000\n","****************************************\n","\n","Evaluation on test-corr:\n","eval_loss : 1.2624\n","eval_accuracy : 0.6398\n","eval_f1_micro : 0.6398\n","eval_precision_micro : 0.6398\n","eval_recall_micro : 0.6398\n","eval_f1_macro : 0.6394\n","eval_precision_macro : 0.6460\n","eval_recall_macro : 0.6805\n","eval_nb_samples : 880.0000\n","****************************************\n","\n","Evaluation on test-misp:\n","eval_loss : 1.3599\n","eval_accuracy : 0.6375\n","eval_f1_micro : 0.6375\n","eval_precision_micro : 0.6375\n","eval_recall_micro : 0.6375\n","eval_f1_macro : 0.6369\n","eval_precision_macro : 0.6445\n","eval_recall_macro : 0.6808\n","eval_nb_samples : 880.0000\n","****************************************\n","\n","Evaluation on test-mae:\n","eval_loss : 1.3263\n","eval_accuracy : 0.6443\n","eval_f1_micro : 0.6443\n","eval_precision_micro : 0.6443\n","eval_recall_micro : 0.6443\n","eval_f1_macro : 0.6435\n","eval_precision_macro : 0.6488\n","eval_recall_macro : 0.6846\n","eval_nb_samples : 880.0000\n","****************************************\n","\n","Evaluation on test-all-mae:\n","eval_loss : 1.2696\n","eval_accuracy : 0.6473\n","eval_f1_micro : 0.6473\n","eval_precision_micro : 0.6473\n","eval_recall_micro : 0.6473\n","eval_f1_macro : 0.6355\n","eval_precision_macro : 0.6341\n","eval_recall_macro : 0.6783\n","eval_nb_samples : 2671.0000\n","****************************************\n","\n"]}]},{"cell_type":"code","metadata":{"id":"UZv8Xs1F674m","executionInfo":{"status":"ok","timestamp":1638890482096,"user_tz":0,"elapsed":12,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"bo7F93aYDNUQ","executionInfo":{"status":"ok","timestamp":1638890482097,"user_tz":0,"elapsed":10,"user":{"displayName":"phattharanan nakwijit","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIIxNe-uXeOWO7avIeHsrg7retrUGk_BMOtjg=s64","userId":"03666715926437036675"}}},"source":[""],"execution_count":81,"outputs":[]}]}