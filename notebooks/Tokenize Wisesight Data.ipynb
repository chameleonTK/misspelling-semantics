{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(fname):\n",
    "    fin = open(fname, encoding=\"utf-8\")\n",
    "    data = []\n",
    "    for line in fin:\n",
    "        d = json.loads(line.strip())\n",
    "        data.append(d)\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as fo:\n",
    "        for idx, d in enumerate(data):\n",
    "            fo.write(json.dumps(d, ensure_ascii=False))\n",
    "            fo.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train: 21628 sents\n",
      "Loaded valid: 2404 sents\n",
      "Loaded test: 2671 sents\n"
     ]
    }
   ],
   "source": [
    "split = [\"train\", \"valid\", \"test\"]\n",
    "wisesight = {}\n",
    "for s in split:\n",
    "    d = load_jsonl(f\"Datasets/WisesightSentiment/{s}.jsonl\")\n",
    "    wisesight[s] = d\n",
    "    print(f\"Loaded {s}: {len(d)} sents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmisp = load_jsonl(f\"Datasets/WisesightSentiment/test-misp.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmisp = load_jsonl(f\"Datasets/WisesightSentiment/few-shot/train-misp-3000.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ฉัน', 'รัก', 'แมว']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "word_tokenize(\"ฉันรักแมว\", engine=\"deepcut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from thai2transformers import preprocess\n",
    "from typing import Collection, Callable\n",
    "import demoji\n",
    "\n",
    "def word_tokenize_deepcut(s):\n",
    "    return word_tokenize(s, engine=\"deepcut\")\n",
    "\n",
    "def _process_transformers(\n",
    "    text: str,\n",
    "    pre_rules: Collection[Callable] = [\n",
    "        preprocess.fix_html,\n",
    "        preprocess.rm_brackets,\n",
    "        preprocess.replace_newlines,\n",
    "        preprocess.rm_useless_spaces,\n",
    "        preprocess.replace_spaces,\n",
    "        preprocess.replace_rep_after,\n",
    "    ],\n",
    "    tok_func: Callable = word_tokenize_deepcut,\n",
    "    post_rules: Collection[Callable] = [preprocess.ungroup_emoji, preprocess.replace_wrep_post],\n",
    "    lowercase: bool = False\n",
    ") -> str:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    for rule in pre_rules:\n",
    "        text = rule(text)\n",
    "    toks = tok_func(text)\n",
    "    for rule in post_rules:\n",
    "        toks = rule(toks)\n",
    "    return toks\n",
    "\n",
    "def replace_emoji(s):\n",
    "    return demoji.replace_with_desc(s, \"\") \n",
    "\n",
    "space_token = \" \"\n",
    "preprocessor=partial(\n",
    "            _process_transformers, \n",
    "            pre_rules = [\n",
    "                replace_emoji,\n",
    "                preprocess.fix_html,\n",
    "                preprocess.rm_brackets,\n",
    "                preprocess.replace_newlines,\n",
    "                preprocess.rm_useless_spaces,\n",
    "#                 preprocess.replace_rep_after\n",
    "            ],\n",
    "            lowercase=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in split:\n",
    "    sents = wisesight[s]\n",
    "    print(f\"Tokenizing {s}:\")\n",
    "    for sent in tqdm(sents, total=len(sents)):\n",
    "        sent[\"tokenized\"] = preprocessor(sent[\"text\"])\n",
    "    save_jsonl(sents, f\"Datasets/WisesightSentiment/tokenized_{s}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with misspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def tokenize_misp_sents(sents):\n",
    "    for sent in tqdm(sents):\n",
    "\n",
    "        engine = \"deepcut\"\n",
    "        sent[\"tokenized\"] = []\n",
    "\n",
    "\n",
    "        segments = []\n",
    "        segwords = []\n",
    "\n",
    "        mispTokens = sorted(sent[\"misp_tokens\"], key=lambda x: x[\"s\"], reverse=False)\n",
    "        text = sent[\"text\"]\n",
    "\n",
    "        lastToken = \"\"\n",
    "        idx, seenTokens = 0, []\n",
    "        for m in mispTokens:\n",
    "            overlapped = False\n",
    "            for p in seenTokens:\n",
    "                if m[\"s\"] < p[\"t\"]:\n",
    "                    overlapped = True\n",
    "\n",
    "            if overlapped:\n",
    "                continue\n",
    "\n",
    "            s = text[idx:m[\"s\"]]\n",
    "            w = text[m[\"s\"]:m[\"t\"]]\n",
    "            t = text[m[\"t\"]:]\n",
    "\n",
    "            idx += len(s)+len(w)\n",
    "            ts = preprocessor(s)\n",
    "\n",
    "\n",
    "            segments.append((s, s))\n",
    "            segments.append((w, m[\"corr\"]))\n",
    "\n",
    "            segwords.append((ts, ts))\n",
    "            segwords.append(([w], [m[\"corr\"]]))\n",
    "\n",
    "            lastToken = t\n",
    "            seenTokens.append(m)\n",
    "\n",
    "        if len(seenTokens)==0:\n",
    "            t = preprocessor(text)\n",
    "            segments = [(text, text)]\n",
    "            segwords = [(t, t)]\n",
    "        else:\n",
    "            t = preprocessor(lastToken)\n",
    "            segments.append((lastToken, lastToken))\n",
    "            segwords.append((t, t))\n",
    "\n",
    "        sent[\"tokenized\"] = preprocessor(sent[\"text\"])    #blindly tokenize\n",
    "    #     sent[\"tokenized\"] = list(itertools.chain(*[s[0] for s in segwords]))\n",
    "    #     sent[\"tokenized\"] = list(itertools.chain(*[s[1] for s in segwords]))\n",
    "    #     sent[\"tokenized\"] = word_tokenize(\"\".join([s[1] for s in segments]), engine=engine)\n",
    "        sent[\"segments\"] = segwords\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2671/2671 [03:51<00:00, 11.55it/s]\n"
     ]
    }
   ],
   "source": [
    "sents = tokenize_misp_sents(testmisp)\n",
    "save_jsonl(sents, f\"Datasets/WisesightSentiment/tokenized_test-misp.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:14<00:00, 11.78it/s]\n"
     ]
    }
   ],
   "source": [
    "sents = tokenize_misp_sents(trainmisp)\n",
    "save_jsonl(sents, f\"Datasets/WisesightSentiment/tokenized_train-misp-3000.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
